# Gradients

Previously, we learned how to compute **partial derivatives** of multivariable functions. These partial derivatives combine to form an important mathematical object called the **gradient**.

Gradients help us understand:

* How a function changes in different directions
* The direction of **steepest ascent**
* The direction of **steepest descent**

These ideas are fundamental to **optimisation** and **machine learning algorithms**.

---

## What Is a Gradient?

For a multivariable function, the **gradient** is a **vector of first-order partial derivatives**.

---

## Gradient of a Two-Variable Function

Let:

$$
f(x, y)
$$

The gradient of ( f ) is denoted by the **nabla symbol** ( \nabla ):

$$
\nabla f(x, y)
$$

and is defined as:

$$
\nabla f(x, y) =
\left(
\frac{\partial f}{\partial x},
\frac{\partial f}{\partial y}
\right)
$$

This is a **vector**, not a scalar.

---

## Interpretation of the Gradient

* The gradient vector at a point points in the **direction of steepest ascent**
* The **magnitude** of the gradient indicates how steep the ascent is
* The **negative gradient** points in the **direction of steepest descent**

Mathematically:

* Direction of steepest ascent:
  $$
  \nabla f(x, y)
  $$

* Direction of steepest descent:
  $$
  -\nabla f(x, y)
  $$

---

## Example 1: Computing a Gradient

Consider the function:

$$
f(x, y) = 2xy^2
$$

---

### Step 1: Partial Derivative with Respect to ( x )

$$
\frac{\partial f}{\partial x} = 2y^2
$$

---

### Step 2: Partial Derivative with Respect to ( y )

$$
\frac{\partial f}{\partial y} = 4xy
$$

---

### Step 3: Gradient Vector

$$
\nabla f(x, y) = (2y^2, 4xy)
$$

---

## Gradient at a Specific Point

Evaluate the gradient at:

$$
(x, y) = (1, 1)
$$

$$
\nabla f(1, 1) = (2, 4)
$$

This vector:

* Starts at the point ( (1, 1) )
* Points in the direction of **maximum increase** of the function
* Indicates the **steepest slope** at that point

---

## Example 2: Radially Symmetric Function

Consider:

$$
f(x, y) = x^2 + y^2
$$

---

### Compute the Gradient

$$
\frac{\partial f}{\partial x} = 2x
$$

$$
\frac{\partial f}{\partial y} = 2y
$$

So,

$$
\nabla f(x, y) = (2x, 2y)
$$

---

### Interpretation

* The gradient always points **away from the origin**
* The further you are from the origin, the larger the gradient
* This reflects that the function increases as we move away from ( (0,0) )

---

## Gradient in Higher Dimensions

For a function of ( n ) variables:

$$
f(x_1, x_2, \dots, x_n)
$$

The gradient is:

$$
\nabla f =
\left(
\frac{\partial f}{\partial x_1},
\frac{\partial f}{\partial x_2},
\dots,
\frac{\partial f}{\partial x_n}
\right)
$$

This gradient lives in **( \mathbb{R}^n )**.

---

## Why Gradients Matter in Machine Learning

In machine learning, we often minimise a **cost function**:

$$
J(\theta_1, \theta_2, \dots, \theta_n)
$$

Training a model means finding:

$$
\min J(\theta_1, \theta_2, \dots, \theta_n)
$$

This is done by moving in the direction of **steepest descent**:

$$
-\nabla J
$$

This idea forms the basis of **gradient descent algorithms**.

---

## Key Takeaways

* The gradient is a **vector of partial derivatives**
* It points in the direction of **steepest ascent**
* The negative gradient points in the direction of **steepest descent**
* Gradients are essential for:

  * Optimisation
  * Machine learning training
  * High-dimensional analysis

---

# Gradients – Visual Explanation

Gradients do two things **at the same time**:

1. Tell us the **direction** in which the function increases the fastest
2. Tell us **how steep** the function is at a point

That’s why gradients are vectors.

---

## Visualising the Gradient at a Point

Consider a function:

$$
f(x, y)
$$

Its gradient is:

$$
\nabla f(x, y) =
\left(
\frac{\partial f}{\partial x},
\frac{\partial f}{\partial y}
\right)
$$

---

### Gradient as an Arrow on the Surface

* Pick a point ( (x, y) ) on the surface
* Compute ( \nabla f(x, y) )
* The gradient is drawn as an **arrow (vector)** at that point

Key ideas:

* The arrow points in the **direction of steepest ascent**
* The **length** of the arrow shows how steep the slope is
* The **negative gradient** points in the direction of steepest descent

---

## Moving Along the Gradient

* Moving **along** ( \nabla f ) → fastest increase in function value
* Moving **opposite** to ( \nabla f ) → fastest decrease

This idea is the foundation of **gradient descent** used in machine learning.

---

## Gradients Over the Entire Surface

Instead of one point, suppose we compute gradients at **every point** on the surface.

This gives a **gradient vector field**.

From above (top view), we see:

* Many arrows
* Each arrow shows the direction of steepest ascent at that location

---

## Arrow Size and Surface Shape

### 1. Flat Regions

* Surface is almost flat
* Change in height is small
* **Gradient vectors are short**

$$
|\nabla f| \approx 0
$$

---

### 2. Steep Regions

* Surface rises or falls quickly
* **Gradient vectors are long**

$$
|\nabla f| \text{ is large}
$$

---

### 3. Near Peaks and Valleys

* Surface flattens near tops and bottoms
* **Gradient vectors become small**
* At an exact peak or valley:

$$
\nabla f = (0, 0)
$$

---

## Gradients and Contour Lines

Contour lines join points with the **same function value**.

Important rule:

* Gradient vectors are **perpendicular** to contour lines
* Closely spaced contour lines ⇒ **steep slope**
* Widely spaced contour lines ⇒ **gentle slope**

Where contour lines are tightly packed, gradients are largest.

---

# Gradient Examples with Clear Explanation

---

## Example 1: Simple Paraboloid

Consider:

$$
f(x, y) = x^2 + y^2
$$

---

### Gradient

$$
\nabla f(x, y) = (2x, 2y)
$$

---

### Visual Interpretation

* At the origin ( (0,0) ):
  $$
  \nabla f(0,0) = (0,0)
  $$
  Slope is flat → smallest vectors

* As we move away from the origin:

  * Height increases
  * Slope increases
  * Vectors get longer

* Gradient vectors point **radially outward**

* The surface looks like a **bowl**

---

## Example 2: Valley Shape

Consider:

$$
f(x, y) = x^2 - y^2
$$

---

### Gradient

$$
\nabla f(x, y) = (2x, -2y)
$$

---

### Visual Interpretation

* Along the ( x )-direction → function increases
* Along the ( y )-direction → function decreases
* The surface has a **saddle point** at the origin

At ( (0,0) ):

$$
\nabla f(0,0) = (0,0)
$$

But this is **not** a minimum or maximum — it’s a saddle point.

---

## Example 3: Slanted Plane

Consider:

$$
f(x, y) = 3x + 2y
$$

---

### Gradient

$$
\nabla f(x, y) = (3, 2)
$$

---

### Visual Interpretation

* Gradient is the **same everywhere**
* Surface is a plane
* Slope does not change
* All arrows have the same length and direction

This means the surface rises uniformly.

---

## Example 4: Circular Ridge

Consider:

$$
f(x, y) = \sqrt{x^2 + y^2}
$$

---

### Gradient

$$
\nabla f(x, y) =
\left(
\frac{x}{\sqrt{x^2 + y^2}},
\frac{y}{\sqrt{x^2 + y^2}}
\right)
$$

---

### Visual Interpretation

* Gradient vectors point outward
* Direction depends on location
* Magnitude is nearly constant (except at the origin)
* Surface looks like a cone

---

## Example 5: Gaussian Hill (ML-style example)

Consider:

$$
f(x, y) = e^{-(x^2 + y^2)}
$$

---

### Gradient

$$
\nabla f(x, y) =
\left(
-2x e^{-(x^2 + y^2)},
-2y e^{-(x^2 + y^2)}
\right)
$$

---

### Visual Interpretation

* Highest point at the origin
* Gradients point **towards the center**
* Near the top → gradients are small
* Further away → gradients increase, then decay

This resembles loss surfaces in ML.

---

## Key Visual Rules to Remember

* Gradient points **uphill**
* Negative gradient points **downhill**
* Bigger arrow ⇒ steeper slope
* Small arrows ⇒ flat regions
* Zero gradient ⇒ peak, valley, or saddle
* Gradients are perpendicular to contour lines

---
