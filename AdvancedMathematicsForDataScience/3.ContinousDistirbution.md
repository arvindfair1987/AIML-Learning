## Continuous Distributions

### Probability Density Functions (PDFs)

In earlier sections, we learned how to compute probabilities using addition and multiplication rules, and how probability distributions such as the **binomial** and **uniform** distributions help us analyse **discrete random variables**.

Examples of discrete random variables include:

* Number of balls drawn
* Number of patients
* Number of cars or wickets
* Number of packets sold

In all these cases, the random variable takes **countable values**.

But what happens when the random variable is **continuous**, such as:

* Time
* Weight
* Height
* Distance

Let us now explore how probability works for **continuous random variables**.

---

## Motivating Example: Daily Commute Time

Suppose you work at a large company with **3,000 employees** in a local office. Let the random variable **X** represent the daily commute time (in minutes) of an employee.

### Probability of an Exact Value

What is the probability that an employee‚Äôs commute time is **exactly 35 minutes**?

Since commute time is a **continuous variable**, it can take infinitely many values:
35, 35.01, 35.001, and so on.

Therefore:

**P(X = 35) = 0**

This does not mean such a commute time is impossible‚Äîit means the probability of observing **one exact value** among infinitely many possibilities is zero.

---

## Probabilities Over Ranges

While probabilities at exact values are zero, probabilities over **intervals** are meaningful.

For example:

* P(35 ‚â§ X ‚â§ 40) represents the probability that an employee‚Äôs commute time lies between 35 and 40 minutes.

For continuous random variables, we always talk about probabilities in terms of **ranges**.

---

## Cumulative Probability for Commute Time

Suppose a fictional dataset gives the following probabilities for commute-time intervals:

| Commute Time (minutes) | Probability |
| ---------------------- | ----------- |
| 0‚Äì25                   | 0.15        |
| 25‚Äì30                  | 0.20        |
| 30‚Äì35                  | 0.30        |
| 35‚Äì40                  | 0.20        |
| 40‚Äì45                  | 0.15        |

We can compute cumulative probabilities as follows:

* P(X ‚â§ 30) = 0.15 + 0.20 = 0.35
* P(X ‚â§ 35) = 0.35 + 0.30 = 0.65
* P(X ‚â§ 40) = 0.65 + 0.20 = 0.85
* P(X ‚â§ 45) = 1.00

---

## Cumulative Distribution Function (CDF)

The **cumulative distribution function (CDF)** is defined as:

F(x) = P(X ‚â§ x)

A CDF plots:

* **X-axis**: values of the random variable
* **Y-axis**: cumulative probability

### Properties of a CDF

1. It is **monotonically non-decreasing**.
2. 0 ‚â§ F(x) ‚â§ 1 for all x.
3. As x approaches the maximum possible value, **F(x) ‚Üí 1**.

---

## Probability Density Function (PDF)

A **probability density function (PDF)** describes how dense the probability is around different values of a continuous random variable.

### Key Idea

* The **area under the PDF curve** over an interval gives the probability of X lying in that interval.

For example:

P(a ‚â§ X ‚â§ b) = Area under the PDF between a and b

---

## Relationship Between PDF and CDF

* The **CDF at x** equals the **area under the PDF** from the smallest possible value of X up to x.

For example:

F(28) = Area under the PDF from X = 20 to X = 28

Thus:

* CDF gives probability directly
* PDF gives probability through **area calculation**

---

## Discrete vs Continuous CDFs

| Discrete Random Variable            | Continuous Random Variable    |
| ----------------------------------- | ----------------------------- |
| CDF plotted as step-like bars       | CDF plotted as a smooth curve |
| CDF changes only at specific values | CDF changes at every value    |
| P(X = x) > 0                        | P(X = x) = 0                  |

For discrete variables, the cumulative probability remains constant between values (e.g., between 3 and 3.9).

For continuous variables, the cumulative probability changes for **every small change in X**.

---

## Uniform Distribution (Continuous)

A commonly observed continuous distribution is the **uniform distribution**.

### Definition

A continuous random variable X is uniformly distributed over [a, b] if the PDF has a **constant value** over this interval.

---

### Example: Uniform Distribution from 0 to 10

Let X be uniformly distributed between 0 and 10.

Since the total area under the PDF must be 1:

Area = length √ó height = 10 √ó h = 1

So:

h = 0.1

Thus, the PDF value for all x between 0 and 10 is **0.1**.

---

### Cumulative Probability Example

Find P(X ‚â§ 0.5):

P(X ‚â§ 0.5) = Area from 0 to 0.5 = 0.1 √ó 0.5 = **0.05**

---

## PDFs vs CDFs in Practice

Although both PDFs and CDFs are valid for continuous variables:

* **PDFs are used more frequently in practice**.
* PDFs make it easier to visually identify patterns such as:

  * Uniformity
  * Symmetry
  * Skewness

CDFs are monotonic and often hide these patterns.

---

## Discrete vs Continuous: Key Insight

If we collect data from 3,000 employees playing the red-ball game:

* X = number of red balls drawn
* X takes only 5 possible values (0 to 4)

Even with large data, X remains **discrete**, and:

P(X = 3) > 0

In contrast, variables such as height, weight, or time are **continuous**, and:

P(X = exact value) = 0

---

## Important Property for Continuous Variables

For a continuous random variable X:

P(X ‚â§ a) = P(X < a)

because:

P(X = a) = 0

---

### Example: Bowler Height

Let X be the height (in cm) of bowlers who took 5-wicket hauls.

If:

F(175.3) = 0.3

Then both statements are correct:

* P(X < 175.3) = 0.3
* P(X ‚â§ 175.3) = 0.3

---

## Summary

* Continuous random variables take infinitely many values.
* Probabilities at exact values are zero.
* Probabilities are computed over intervals using PDFs and CDFs.
* CDFs show cumulative probability; PDFs show probability density.
* PDFs are preferred for visual interpretation.

Next, we will study one of the most important continuous distributions: the **normal distribution**.

## Normal Distribution

You have seen how probability distributions for **continuous random variables** differ from those for discrete random variables. Among all continuous distributions, one distribution appears **most frequently in nature and data analysis**‚Äîthe **normal distribution**.

It is also known as:

* **Gaussian distribution**
* **Bell curve**

---

## Introduction to the Normal Distribution

The **normal distribution** is one of the most commonly used probability density functions for continuous random variables.

It is typically used when:

* Most values are concentrated around a central value
* Extreme values are rare
* The distribution is **symmetric** about the centre

Examples of variables that often follow a normal distribution include:

* Heights of adults
* IQ scores
* Measurement errors
* Weight of manufactured products (e.g., Coca-Cola bottles)

The normal distribution also plays a key role in advanced topics such as the **Central Limit Theorem**.

---

## Shape and Key Properties

A normal distribution has the following properties:

1. It is **symmetric** about its mean
2. The **mean, median, and mode coincide** at the centre
3. The curve is bell-shaped
4. The total area under the curve equals **1**
5. The tails of the distribution are **light**, meaning extreme values are rare

---

## Parameters of a Normal Distribution

A normal distribution is completely described by two parameters:

* **Mean (Œº)**: Determines the centre of the distribution
* **Standard deviation (œÉ)**: Determines the spread of the distribution

It is denoted as:

X ~ N(Œº, œÉ¬≤)

---

## The 1‚Äì2‚Äì3 Rule (Empirical Rule)

For a normal distribution with mean Œº and standard deviation œÉ:

| Interval | Approximate Probability |
| -------- | ----------------------- |
| Œº ¬± 1œÉ   | 68%                     |
| Œº ¬± 2œÉ   | 95%                     |
| Œº ¬± 3œÉ   | 99.7%                   |

This is known as the **1‚Äì2‚Äì3 rule** or the **Empirical Rule**.

It tells us that almost all observations lie close to the mean.

---

## Example 1: Probability Between Two Values

Let X be a normally distributed random variable with:

* Mean Œº = 35
* Standard deviation œÉ = 5

### Find P(25 ‚â§ X ‚â§ 45)

Here:

* Œº ‚àí 2œÉ = 25
* Œº + 2œÉ = 45

From the 1‚Äì2‚Äì3 rule:

P(Œº ‚àí 2œÉ ‚â§ X ‚â§ Œº + 2œÉ) ‚âà **95%**

---

## Example 2: Probability Between Unequal Ranges

Find P(25 ‚â§ X ‚â§ 50)

Rewrite the limits:

* 25 = Œº ‚àí 2œÉ
* 50 = Œº + 3œÉ

Break the range into two parts:

### Part 1: Œº ‚àí 2œÉ to Œº

Half of 95% = **47.5%**

### Part 2: Œº to Œº + 3œÉ

Half of 99.7% = **49.85%**

### Final Probability

P(Œº ‚àí 2œÉ ‚â§ X ‚â§ Œº + 3œÉ)
= 47.5% + 49.85%
= **97.35%**

---

## Example 3: Probability Less Than a Value

Find P(X < 40)

Here:

* 40 = Œº + 1œÉ

From the 1‚Äì2‚Äì3 rule:

* P(X < Œº) = 50%
* P(Œº ‚â§ X ‚â§ Œº + 1œÉ) = 34%

Therefore:

P(X < 40) = 50% + 34% = **84%**

---

## Real-Life Interpretation of the 1‚Äì2‚Äì3 Rule

Suppose the weight of a loaf of bread is normally distributed with:

* Mean = 100 g
* Standard deviation = 1 g

Then:

* About **68%** of loaves weigh between **99 g and 101 g**
* About **95%** weigh between **98 g and 102 g**
* About **99.7%** weigh between **97 g and 103 g**

---

## Normal Distribution in Real Data

Many naturally occurring variables are approximately normally distributed.

For example, if we measure the heights of 50 adult men and plot their probability density function, the resulting curve is typically **bell-shaped**, closely resembling a normal distribution.

---

## Summary

* The normal distribution is the most common continuous distribution
* It is symmetric and centred at the mean
* Mean, median, and mode are equal
* The 1‚Äì2‚Äì3 rule helps estimate probabilities quickly
* Most real-world measurements cluster close to the mean

Next, we will study an important special case of the normal distribution: the **standard normal distribution**.

Here‚Äôs a **cleaned, corrected, and well-structured version** of your notes, rewritten in clear academic language and proper flow, while keeping the teaching intent intact. I‚Äôve also fixed numerical slips, repetition, and awkward phrasing.

---

## Z-Score and the Standard Normal Distribution

As you learnt in the previous section, when working with a normal distribution, the *actual values* of the mean ( \mu ) and standard deviation ( \sigma ) are not as important as **how far a value lies from the mean**.

More specifically, what matters is **how many standard deviations away from the mean** a random variable is.

This idea leads us to the concept of the **Z-score**.

---

## Introduction to Z-Score

In general, we are interested in finding how many standard deviations away from the mean a random variable is.

### Example

Suppose:

* Mean $( \mu = 35 )$
* Standard deviation $( \sigma = 5 )$
* Random variable $( X = 43.25 )$

The difference between ( X ) and the mean is:
$$[
43.25 - 35 = 8.25
]$$

So, the value is **8.25 units away** from the mean.
However, to express this distance in standard terms, we divide by the standard deviation:
$$[
\frac{8.25}{5} = 1.65
]$$

This means that **43.25 lies 1.65 standard deviations above the mean**.

This value, **1.65**, is called the **Z-score** of the random variable.

---

## Definition of Z-Score (Standardized Normal Variable)

The Z-score is defined as:
$$[
Z = \frac{X - \mu}{\sigma}
]$$

The variable ( Z ) is called the **standardized normal variable**.

It tells us:

* How far a value is from the mean
* In units of standard deviation
* Along with the direction (positive or negative)

---

## Understanding Z Through Probability

The Z-score allows us to translate probabilities involving the random variable ( X ) into probabilities involving ( Z ).

Some key equivalences:

* The probability of
  $$[
  -2 \le Z \le 2
  ]$$
  is the same as the probability of
  $$[
  \mu - 2\sigma \le X \le \mu + 2\sigma
  ]$$
  which, from the **Empirical (1-2-3) Rule**, is approximately **95%**.

* The probability of
  $$[
  -2 \le Z \le 3
  ]$$
  corresponds to
  $$[
  \mu - 2\sigma \le X \le \mu + 3\sigma
  ]$$
  which equals:
  $$[
  47.5% + 49.85% = 97.35%
  ]$$

* The probability of
  $$[
  Z \le 1
  ]$$
  is the same as the probability of
  $$[
  X \le \mu + \sigma
  ]$$
  which is approximately **84%**.

Through these examples, we see that the standardized variable ( Z ) is often **more informative** than the original variable ( X ).

---

## Why Standardization Is Important

* A **positive Z-score** indicates the value lies to the **right of the mean**, resulting in a **higher cumulative probability**.
* A **negative Z-score** indicates the value lies to the **left of the mean**, resulting in a **lower cumulative probability**.

The unstandardized variable ( X ) does not directly provide this insight.

Because of this, probabilities are commonly calculated using the **distribution of Z rather than X**.

This distribution of the standardized variable ( Z ) is known as the **Standard Normal Distribution**.

---

## The Z-Table and Cumulative Probability

So far, we have seen probabilities corresponding to integer Z-values such as ¬±1, ¬±2, and ¬±3.

But Z is a **continuous variable**. What if:
$$[
Z = 0.68?
]$$

To handle such cases, mathematicians have pre-calculated cumulative probabilities for all Z-values and compiled them into a table known as the **Z-table**.

The Z-table gives the **cumulative probability**:
$$[
P(Z \le z)
]$$

---

## How to Read the Z-Table

To find the cumulative probability for:
$$[
Z = 0.68
]$$

1. Use the first decimal place (**0.6**) to find the row
2. Use the second decimal place (**0.08**) to find the column
3. Locate the intersection of the row and column

From the table:
$$[
P(Z \le 0.68) = 0.7517
]$$

---

## Applying the Z-Table to Real-World Problems

### Example 1: Less Than a Given Value

Suppose commute times are normally distributed with:

* $( \mu = 35 )$
* $( \sigma = 5 )$

Find the probability that an employee‚Äôs commute time is **less than 43.25 minutes**.

**Step 1: Convert X to Z**
$$[
Z = \frac{43.25 - 35}{5} = 1.65
]$$

**Step 2: Use the Z-table**
$$[
P(Z \le 1.65) \approx 0.95
]$$

So, there is a **95% probability** that the commute time is less than 43.25 minutes.

---

### Example 2: Probability Between Two Values

Find the probability that:
$$[
25.2 \le X \le 44.8
]$$

Convert both values to Z-scores:
$$[
Z_1 = -1.96, \quad Z_2 = 1.96
]$$

From the Z-table:
$$[
P(Z \le 1.96) = 0.975
]$$
$$[
P(Z \le -1.96) = 0.025
]$$

Therefore:
$$[
P(-1.96 \le Z \le 1.96) = 0.975 - 0.025 = 0.95
]$$

So, the required probability is **95%**.

---

## Key Takeaways

* The standardized random variable is given by:
  $$[
  Z = \frac{X - \mu}{\sigma}
  ]$$
* It tells us **how many standard deviations away from the mean** a value lies
* Probabilities for Z can be obtained using the **Z-table**
* The Z-table provides **cumulative probabilities**
* Standardization allows us to use a **single universal distribution** for all normal variables

---

**Z-Table Reference:**
Standard Normal Distribution Table
[https://math.arizona.edu/~rsims/ma464/standardnormaltable.pdf](https://math.arizona.edu/~rsims/ma464/standardnormaltable.pdf)


Here‚Äôs a **polished, well-sequenced continuation of your notes**, with clearer math notation, stronger intuition, and examples aligned with the PSU links you mentioned. I‚Äôve kept it lecture-friendly and concept-first.

---

## Cumulative Probability Using the Normal Distribution Formula

So far, we have relied on the **Z-table** to compute cumulative probabilities.
Alternatively, the cumulative probability for a standard normal variable can be computed using the **normal distribution function**.

### Mathematical Definition

The cumulative distribution function (CDF) of a standard normal random variable ( Z ) is given by:

[
F(Z) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{Z} e^{-t^2/2} , dt
]

This integral computes:
[
P(Z \le Z)
]

üìå **Important note:**

* This integral **does not have a closed-form solution**
* That is why we use **tables** or **software tools** (Excel, Python, R, etc.)

---

## Using Excel to Find Cumulative Probability

### Example 1: Standard Normal Variable

Suppose you want to find the cumulative probability for:
[
Z = 1.5
]

In **Excel**, use:

```
= NORM.S.DIST(1.5, TRUE)
```

### Syntax Explanation

```
= NORM.S.DIST(z, TRUE)
```

* `z` ‚Üí Z-score
* `TRUE` ‚Üí returns **cumulative probability** ( P(Z \le z) )
* `FALSE` ‚Üí returns **probability density** (height of the curve at z)

So,
[
P(Z \le 1.5) \approx 0.9332
]

---

## Finding Probability Without Standardising

You don‚Äôt always need to convert X into Z manually. Excel can do this directly.

### Example 2: Original Normal Variable

Suppose:

* ( X \sim N(\mu = 35, \sigma = 5) )
* Find ( P(X \le 30) )

In **Excel**, use:

```
= NORM.DIST(30, 35, 5, TRUE)
```

### General Syntax

```
= NORM.DIST(x, mean, standard_dev, TRUE)
```

This directly computes:
[
P(X \le x)
]

üìå Behind the scenes, Excel **standardises X automatically** and then uses the standard normal distribution.

---

## Effect of œÉ on the Shape of the Normal Distribution

A useful and intuitive observation is how the **standard deviation (œÉ)** affects the shape of the normal curve.

### Key Intuition

* **Mean (Œº)** ‚Üí controls the **centre** of the distribution
* **Standard deviation (œÉ)** ‚Üí controls the **spread / width**

### What Happens When œÉ Changes?

* **Small œÉ**

  * Curve is **narrow and tall**
  * Values are tightly clustered around the mean
* **Large œÉ**

  * Curve is **wide and flat**
  * Values are more spread out from the mean

This intuition applies to **any distribution**, not just the normal distribution.

üìå A wider curve implies:

* More values far from the mean
* Hence, a **larger standard deviation**

---

## Other Important Continuous Probability Distributions

Besides the normal distribution, several other continuous distributions are commonly used:

### 1. Exponential Distribution

* Models **time until an event occurs**
* Example: time until a machine fails
* Memoryless property

### 2. Gamma Distribution

* Generalisation of the exponential distribution
* Models waiting time until **multiple events** occur

### 3. Chi-Squared Distribution

* Used in **hypothesis testing** and **variance estimation**
* Appears naturally when squaring standard normal variables

These distributions will be explored later with applications.

---

## Z-Table (PDF)

You can download the Z-table in PDF format and keep it handy:

üìå It is recommended to:

* Keep it open on a **secondary screen**
* Or take a **printout** for quick reference during problem solving

This will be especially useful when solving numerical problems later in the module.

---

## Intuition from the PSU Links (Conceptual Summary)

Based on the referenced PSU material:

### Key Insights Reinforced

1. **Standardisation unifies all normal distributions**

   * Any normal variable can be mapped to the standard normal distribution
   * This allows a **single probability table** to be used universally

2. **CDF represents area under the curve**

   * Probability is always interpreted as **area**
   * Cumulative probability = area from $( -\infty )$ to a point

3. **Symmetry of the normal distribution**

   * Area to the left of 0 is 0.5
   * $( P(Z \le -z) = 1 - P(Z \le z) )$

4. **Software tools approximate the same integral**

   * Z-table, Excel, Python ‚Üí all approximate the same CDF integral

---

## Summary of Learnings

* The cumulative probability of a normal variable can be computed using:

  * Z-table
  * Mathematical integral (CDF)
  * Software tools (Excel, Python)
* Excel functions:

  * `NORM.S.DIST()` ‚Üí standard normal
  * `NORM.DIST()` ‚Üí general normal
* Standard deviation controls the **spread** of the curve
* Standardisation allows probability calculations to be **simple and universal**



I found the main **Lesson 15 page for STAT 414**, but the specific *‚Äú15.4‚Äù* page redirects back to the course home, so we can‚Äôt fetch that exact sub-section directly. However, **Lesson 15 clearly outlines what this set of sub-topics covers**‚Äîand I can create a **PSU-style summary of *Gamma distributions***, since *15.4* appears to be the start of that section. ([PennState: Statistics Online Courses][1])

Below are **clear lecture-style notes** similar to PSU‚Äôs presentation for *Gamma distributions* (which is what Lesson 15.4 is about when you follow the navigation links).

---

# **Gamma Distributions (STAT 414 ‚Äî Lesson 15)**

### **Overview & Motivation**

In Lesson 15, the course shifts from the exponential distribution to more general continuous distributions like:

* **Exponential**
* **Gamma**
* **Chi-square**

These are important models for waiting times and shape-controlled continuous data. ([PennState: Statistics Online Courses][1])

---

## **1) From Exponential to Gamma**

### **Waiting for One Event**

* In a Poisson process with rate (\lambda) (events per unit time), the waiting time until the **first event** is exponential with mean (\theta=1/\lambda).
* Exponential: a *special case* of Gamma when you are waiting for just **one** event. ([PennState: Statistics Online Courses][2])

### **Waiting for Œ± Events**

Now suppose you want the waiting time (W) until the **Œ±-th event** occurs in a Poisson process.

* Let (W) be the waiting time until the Œ±-th event.
* Then (W) follows what is called a **Gamma distribution**.

This generalizes the exponential *waiting time* idea to multiple events. ([PennState: Statistics Online Courses][2])

---

## **2) Deriving the Gamma Distribution**

We can find the distribution of (W) by thinking about the events that didn‚Äôt happen:

* The waiting time (W) is greater than (w) if **fewer than Œ± events** occur in time (w).
* That means (0, 1, ..., (\alpha-1)) events occurred.

Using the Poisson probabilities for (k) events in time (w):

$$[
P(\text{exactly } k\ \text{events in }[0,w]) = \frac{(\lambda w)^k e^{-\lambda w}}{k!}
]$$

Thus we get:

$$[
F(w) = 1 - \sum_{k=0}^{\alpha-1} \frac{(\lambda w)^k e^{-\lambda w}}{k!}
]$$

Differentiating this with respect to (w) yields the **PDF** of the waiting time. After simplification and re-parameterization using (\theta = 1/\lambda):

---

## **3) Gamma PDF (Probability Density Function)**

A random variable (W) has a **Gamma distribution** with parameters (\alpha>0) (shape) and (\theta>0) (scale) if:

$$[
f(w)=\frac{1}{\theta^\alpha \Gamma(\alpha)} w^{\alpha-1} e^{-w/\theta},\quad w>0
]$$

Where:

* (\alpha) determines the **shape** of the distribution
* (\theta) is the **scale parameter**
* (\Gamma(\alpha)) is the **gamma function**, a generalization of factorial:
  $$[
  \Gamma(\alpha)=\int_0^\infty t^{\alpha-1}e^{-t}dt
  ]$$

This PDF integrates to 1 and is valid for all (w>0). ([PennState: Statistics Online Courses][2])

---

## **4) Special Cases**

* If (\alpha = 1), the Gamma distribution reduces to the **Exponential distribution**:
  $$[
  f(w)=\frac{1}{\theta}e^{-w/\theta}
  ]$$
* The **Chi-square distribution** is a special case of Gamma with $(\theta=2)$ and $(\alpha=k/2)$ for (k) degrees of freedom. ([PennState: Statistics Online Courses][1])

---

## **5) Effects of Parameters**

### **Effect of Œ∏ (Scale)**

* Larger $(\theta)$ ‚Äústretches‚Äù the distribution to the right ‚Üí longer waiting times are more probable.
* Smaller $(\theta)$ compresses the curve ‚Üí outcomes cluster closer to zero.

For fixed $(\alpha)$, increasing $(\theta)$ pushes the mass of the distribution to larger values. ([PennState: Statistics Online Courses][2])

### **Effect of Œ± (Shape)**

* When $(\alpha)$ increases, the distribution becomes more **spread out and right-shifted**.
* The curve also becomes smoother and more symmetric for large $(\alpha)$.
* Intuition: waiting for more events means you generally wait longer on average. ([PennState: Statistics Online Courses][2])

---

## **6) Properties of the Gamma Distribution**

If $(W\sim\text{Gamma}(\alpha,\theta))$:

* **Mean:**
  $$[
  E(W) = \alpha\theta
  ]$$

* **Variance:**
  $$[
  \operatorname{Var}(W) = \alpha\theta^2
  ]$$

* **Moment Generating Function:**
  $$[
  M_W(t)=\left(1-\theta t\right)^{-\alpha},\quad t<1/\theta
  ]$$

These results come from integrating the PDF multiplied by appropriate powers of (w). ([PennState: Statistics Online Courses][2])

---

## **7) Why Gamma Matters**

* **Waiting times:** models the sum of multiple exponential waiting times.
* **Poisson processes:** natural in queuing, reliability, and lifetime analysis.
* **Chi-square connection:** a building block for many statistical tests and distributions. ([PennState: Statistics Online Courses][1])

---

## **8) Example Interpretation**

Suppose events occur at a rate (\lambda = 2) per hour:

* Waiting for the **first event** follows $( \text{Exponential}(\theta=0.5))$.
* Waiting for the **third event** follows:
  $$[
  W \sim \text{Gamma}(\alpha=3,\theta=0.5)
  ]$$

You can use the Gamma PDF or CDF to compute the probability of, say, waiting less than 1 hour for the third event. This involves evaluating or integrating the gamma density up to that point. ([PennState: Statistics Online Courses][2])

---

## **9) Summary**

| Distribution | Parameters        | When Used                 |
| ------------ |-------------------| ------------------------- |
| Exponential  | $(\theta)$        | Waiting for first event   |
| Gamma        | $(\alpha,\theta)$ | Waiting for Œ± events      |
| Chi-Square   | Special Gamma     | Used in tests & inference |

Gamma distributions generalize exponential waiting times and connect deeply with other important distributions like chi-square and Erlang. ([PennState: Statistics Online Courses][1])

Perfect ‚Äî here are **PSU-style practice problems on the Gamma distribution**, written in the **same intuition-first, exam-ready style** as STAT 414.
I‚Äôve included **clear solutions and interpretation**, just like the Penn State notes.

---

# **Gamma Distribution ‚Äì Practice Problems with Solutions**

---

## **Problem 1: Identifying the Correct Distribution**

Events occur according to a Poisson process at a rate of **2 events per hour**.

### (a)

What is the distribution of the waiting time until the **first event**?

### (b)

What is the distribution of the waiting time until the **third event**?

---

### **Solution**

**(a)**
The waiting time until the **first event** in a Poisson process follows an **Exponential distribution**.

$$[
W_1 \sim \text{Exponential}(\lambda = 2)
]$$

or equivalently,

$$[
W_1 \sim \text{Gamma}(\alpha = 1,\ \theta = 1/2)
]$$

---

**(b)**
The waiting time until the **third event** follows a **Gamma distribution** with:

* Shape: (\alpha = 3)
* Scale: (\theta = 1/\lambda = 1/2)

$$[
W_3 \sim \text{Gamma}(3, 0.5)
]$$

‚úî **Key insight:**
Waiting for the Œ±-th event ‚áí **Gamma(Œ±, Œ∏)**

---

## **Problem 2: Mean and Variance**

Let
$$[
X \sim \text{Gamma}(\alpha = 4,\ \theta = 3)
]$$

Find:

1. $(E(X))$
2. $(\operatorname{Var}(X))$

---

### **Solution**

For a Gamma distribution:

$$[
E(X) = \alpha\theta
\quad\text{and}\quad
\operatorname{Var}(X) = \alpha\theta^2
]$$

---

### Calculations

* Mean:
  $$[
  E(X) = 4 \times 3 = 12
  ]$$

* Variance:
  $$[
  \operatorname{Var}(X) = 4 \times 3^2 = 36
  ]$$

---

## **Problem 3: Recognizing Special Cases**

Let
$$[
Y \sim \text{Gamma}\left(\alpha = \frac{k}{2},\ \theta = 2\right)
]$$

What well-known distribution does (Y) follow?

---

### **Solution**

This is the **Chi-Square distribution** with (k) degrees of freedom:

$$[
Y \sim \chi^2_k
]$$

‚úî **Key connection:**

$$[
\chi^2_k \equiv \text{Gamma}\left(\frac{k}{2}, 2\right)
]$$

This relationship is fundamental in hypothesis testing.

---

## **Problem 4: PDF Interpretation**

Suppose the waiting time (in hours) for the **fifth event** in a Poisson process with rate (\lambda = 1) follows a Gamma distribution.

### (a)

Write the PDF of the waiting time.

### (b)

Explain the meaning of the shape parameter.

---

### **Solution**

### (a) PDF

Here:

* $(\alpha = 5)$
* $(\theta = 1)$

$[
f(w) = \frac{1}{\Gamma(5)} w^{4} e^{-w}, \quad w>0
]$

Since (\Gamma(5)=4! = 24):

$[
f(w) = \frac{1}{24} w^4 e^{-w}
]$

---

### (b) Interpretation

* The shape parameter $(\alpha = 5)$ represents **waiting for the 5th event**.
* Larger $(\alpha)$ ‚áí longer expected waiting time and more right-skewed distribution.

---

## **Problem 5: Effect of Parameters (Conceptual)**

Which of the following will produce a **wider** Gamma distribution?

1. $(\text{Gamma}(3, 1))$
2. $(\text{Gamma}(3, 4))$

Explain.

---

### **Solution**

Both have the same **shape** $((\alpha = 3))$, but different **scale** values.

* Mean:
  $$[
  E(X) = \alpha\theta
  ]$$
* Variance:
  $$[
  \operatorname{Var}(X) = \alpha\theta^2
  ]$$

---

### Comparison

* Gamma(3, 1):
  Mean = 3, Variance = 3

* Gamma(3, 4):
  Mean = 12, Variance = 48

‚úî **Answer:**
**Gamma(3, 4)** is wider because a larger (\theta) stretches the distribution.

---

## **Problem 6: Linking Exponential and Gamma**

Explain why the sum of **independent exponential random variables** follows a Gamma distribution.

---

### **Solution**

* Each exponential variable represents waiting for **one event**.
* The sum of Œ± independent exponential waiting times equals the waiting time until the **Œ±-th event**.
* Therefore, the sum follows a **Gamma(Œ±, Œ∏)** distribution.

‚úî This is why Gamma naturally appears in **queueing theory, reliability, and survival analysis**.

---

## **Exam-Ready Summary**

* **Gamma distribution** models waiting time until the Œ±-th event.
* **Exponential** is a special case $(Œ± = 1)$.
* **Chi-square** is another special case.
* Mean and variance scale linearly and quadratically with $(\theta)$.
* Shape parameter controls skewness; scale controls spread.



---

## **Summary: Continuous Random Variables & Normal Distribution**

For **continuous random variables**, the probability of observing an **exact value** is zero. Hence, probabilities are always discussed **over intervals**, not at single points.
For example, while the probability of an employee‚Äôs commute time being **exactly 35 minutes** is 0, the probability of it being **between 35 and 40 minutes** can be positive.

Because of this, **PDFs (Probability Density Functions)** and **CDFs (Cumulative Distribution Functions)** are used instead of bar charts (which are suitable only for discrete variables).

* A **PDF** shows how probability is distributed across values; probabilities are obtained by finding the **area under the curve** over an interval.
* A **CDF** directly gives the **cumulative probability** $(P(X \le x))$ at a point.
* PDFs are more commonly used in practice because they make **patterns and shapes** (such as symmetry or uniformity) easier to recognize.

---

## **Normal Distribution**

The **normal distribution** is one of the most widely used continuous distributions. It is:

* Symmetric
* Bell-shaped
* Characterized by mean $((\mu))$ and standard deviation $((\sigma))$
* Mean, median, and mode all lie at the centre

Many natural phenomena (heights, weights, measurement errors) approximately follow a normal distribution.

---

## **Empirical (1‚Äì2‚Äì3) Rule**

For a normally distributed random variable:

* **68%** of values lie within **$¬±1œÉ$** of the mean
* **95%** of values lie within **$¬±2œÉ$** of the mean
* **99.7%** of values lie within **$¬±3œÉ$** of the mean

This rule allows quick probability estimation without knowing exact parameter values.

---

## **Key Insight: Probabilities Independent of Œº and œÉ**

To find probabilities, you do **not** need the actual values of $(\mu)$ and $(\sigma)$; you only need to know **how many standard deviations away from the mean** a value lies.

For example:
$$[
P(X < \mu + \sigma) = 0.5 + \frac{0.68}{2} = 0.84
]$$

So, **for any normal distribution**,
$$[
P(X < \mu + \sigma) = 84%
]$$

---

## **Z-Score (Standardisation)**

To measure distance from the mean in standard units, we define the **Z-score**:

$$[
Z = \frac{X - \mu}{\sigma}
]$$

* Z tells us **how many standard deviations** a value is away from the mean.
* Positive Z ‚Üí value lies to the right of the mean
* Negative Z ‚Üí value lies to the left of the mean

This transformation converts any normal variable into a **standard normal variable** with:

* Mean = 0
* Standard deviation = 1

---

## **Using the Z Table**

The **Z table** provides cumulative probabilities $(P(Z \le z))$.

Example:

* For (Z = 0.68), the table entry **0.7517** gives:
  $$[
  P(Z \le 0.68) = 0.7517
  ]$$

To find probabilities over intervals:
$$[
P(-1.65 < Z < 1.65) = P(Z < 1.65) - P(Z < -1.65) = 0.95 - 0.05 = 0.90
]$$

---

## **Why This Matters**

Understanding PDFs, CDFs, normal distributions, and Z-scores forms the **foundation of statistics**, which underpins:

* Statistical inference
* Hypothesis testing
* Machine learning algorithms
* Data science modeling

These concepts connect probability theory to real-world data analysis and decision-making.

---

‚Äú**Under- or over-estimates the distance by less than 500 km**‚Äù means:

$$[
-500 \le X \le 500
]$$

where (X) is the error in km.

---

### Given

* $(X \sim N(\mu=0,\ \sigma=1000))$

We want:
$$[
P(|X| < 500) = P(-500 < X < 500)
]$$

---

### Step 1: Standardize (convert to Z-scores)

$$[
Z = \frac{X - \mu}{\sigma}
]$$

Lower bound:
$$[
Z = \frac{-500 - 0}{1000} = -0.5
]$$

Upper bound:
$$[
Z = \frac{500 - 0}{1000} = 0.5
]$$

So:
$$[
P(-500 < X < 500) = P(-0.5 < Z < 0.5)
]$$

---

### Step 2: Use the standard normal table

From Z-tables:
$$[
P(Z < 0.5) = 0.6915
]$$
$$[
P(Z < -0.5) = 0.3085
]$$

Subtract:
$$[
P(-0.5 < Z < 0.5) = 0.6915 - 0.3085 = 0.383
]$$

---

### ‚úÖ Final Answer

$$[
\boxed{P(|X| < 500) \approx 0.383}
]$$

---


The normal distribution, aka the Gaussian distribution, was discovered by Carl Friedrich Gauss in 1809. Gauss was trying to create a probability distribution for astronomical errors. Astronomical errors are the errors that were made by astronomers while observing phenomena such as distances in space.


For example, Gauss found that an astronomer trying to estimate the distance between Earth and Uranus always makes an error. This error is normally distributed, with ¬µ = 0 km and œÉ = 1,000 km.

Based on the information above, what is the probability of the astronomer overestimating the distance by 2,330 km or more?

(You can refer an external Z-table for help here)

EDMLDS - C1M1S3 - Graded Questions - Q2

Yes ‚Äî this solution is **completely correct** ‚úÖ
You‚Äôve done every step properly.

### Quick confirmation

* Distribution: (X \sim N(0, 1000)) ‚úîÔ∏è
* Z-score: (z = 2.33) ‚úîÔ∏è
* Z-table value: (P(Z \le 2.33) = 0.9901) ‚úîÔ∏è
* Tail probability:
  $$[
  P(X \ge 2330) = 1 - 0.9901 = 0.0099
  ]$$

### ‚úÖ Final Result
$$P(X‚â•2330)‚âà0.0099 (or 0.99%)$$