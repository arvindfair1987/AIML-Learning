## Motivation: Why We Need Theory Beyond Experiments

In earlier sessions, probabilities were estimated **experimentally** by asking people to play a game (e.g., the UpGrad red–blue ball game). From the observed frequencies, we constructed:

* A **frequency distribution (histogram)**
* An **experimental probability distribution**

While this approach works, it has clear limitations:

* Requires **many repetitions** for accuracy
* Needs **time, resources, and participants**
* Not feasible for every new setup (e.g., changing the number of balls)

This motivates the need for a **theoretical method** to compute probabilities *without* conducting experiments.

---

## Casino Profitability and Expected Value (Recap)

Casinos ensure profitability by designing games with a **negative expected value (EV)** for the player.

* If EV < 0 → player loses money on average
* If EV > 0 → player gains money on average

To evaluate EV, we must first know the **probability distribution** of outcomes.

---

## Can We Find Probabilities Without Experiments?

Yes. If the experiment structure is known, probabilities can be computed using **probability rules**:

* Multiplication rule
* Addition rule
* Combinatorics

This allows us to **generalize** results without repeating experiments.

---

## Running Example: Red–Blue Ball Experiment

### Experiment Setup

* Bag contains **3 red balls** and **2 blue balls**
* Probability of drawing a red ball:
  $$[ P(R) = \frac{3}{5} = 0.6 ]$$
* Probability of drawing a blue ball:
  $$[ P(B) = \frac{2}{5} = 0.4 ]$$
* Balls are **replaced after each draw** (independent trials)
* Number of trials: **4 draws**

Define random variable:

> **X = number of red balls drawn in 4 trials**

---

## Multiplication Rule of Probability

If two events are **independent**, then:

$$[ P(A \cap B) = P(A) \times P(B) ]$$

This rule allows us to compute probabilities of **sequences of outcomes**.

---

## Calculating Probabilities of X

### Case 1: Probability of 4 Red Balls (X = 4)

$$[ P(X=4) = 0.6^4 = 0.1296 ]$$

(Only one such sequence: RRRR)

---

### Case 2: Probability of 0 Red Balls (X = 0)

$$[ P(X=0) = 0.4^4 = 0.0256 ]$$

(Only one such sequence: BBBB)

---

### Case 3: Probability of 3 Red Balls (X = 3)

* Total ways to place 1 blue ball among 4 positions:
  $$[ \binom{4}{1} = 4 ]$$
* Probability of each sequence:

$$[ 0.4 \times 0.6^3 = 0.0864 ]$$

* Total probability:

$$[ P(X=3) = 4 \times 0.0864 = 0.3456 ]$$

(Addition rule applied to **mutually exclusive** sequences)

---

### Case 4: Probability of 1 Red Ball (X = 1)

* Total ways to place 1 red ball among 4 draws:

$$[ \binom{4}{1} = 4 ]$$

* Probability of each sequence:

$$[ 0.6 \times 0.4^3 = 0.0384 ]$$

* Total probability:

$$[ P(X=1) = 4 \times 0.0384 = 0.1536 ]$$

---

### Case 5: Probability of 2 Red Balls (X = 2)

* Number of sequences:

$$[ \binom{4}{2} = 6 ]$$

* Probability of each sequence:

$$[ 0.6^2 \times 0.4^2 = 0.0576 ]$$

* Total probability:

$$[ P(X=2) = 6 \times 0.0576 = 0.3456 ]$$

---

## Final Probability Distribution

| X (No. of Red Balls) | P(X)   |
| -------------------- | ------ |
| 0                    | 0.0256 |
| 1                    | 0.1536 |
| 2                    | 0.3456 |
| 3                    | 0.3456 |
| 4                    | 0.1296 |

(Check: Probabilities sum to 1 ✔)

---

## Visualization

* X-axis: Number of red balls
* Y-axis: Probability
* The distribution is **symmetric around the mean**
* Middle values (X = 2, 3) have higher probabilities

---

## Experimental vs Theoretical Distribution

* **Experimental distribution**: Obtained from limited trials
* **Theoretical distribution**: Calculated using probability rules

### Why are they different?

* Experiments were conducted only a **small number of times**
* Random variation affects results

### Key Insight

> As the number of experiments increases, the **experimental distribution converges to the theoretical distribution** (Law of Large Numbers).

---

## Important Corrections

* **Addition rule applies to mutually exclusive events**, not independent events
* Correct heading:
  **Observed Probability Distribution vs Theoretical Probability Distribution**

---

## Key Takeaways

* Probabilities can be computed **without experiments** using theory
* Multiplication rule handles sequences
* Addition rule combines mutually exclusive outcomes
* This framework naturally leads to the **Binomial Distribution**

---

## What’s Next?

We now generalize this idea formally using the **Binomial Distribution**, which provides a direct formula to compute:

$$[ P(X = k) = \binom{n}{k} p^k (1-p)^{n-k} ]$$

without enumerating all cases.

## Binomial Distribution – Part 1

### Motivation

Earlier, we calculated probabilities for the red–blue ball game **theoretically** and compared them with experimental results. The key advantage of the theoretical approach is that:

* We do **not** need to repeat experiments many times
* Probabilities can be calculated using **pen-and-paper mathematics**
* The same framework can be **generalised** to many similar problems

In this section, we generalise the earlier setup and arrive at the **binomial distribution**.

---

### Experiment Setup (Generalised)

* An experiment consists of **4 independent trials**
* In each trial:

  * Probability of drawing a **red ball** = **p**
  * Probability of drawing a **blue ball** = **1 − p**
* A ball is **replaced** after every draw, so probabilities remain constant

We define a random variable:

> **X = number of red balls drawn in 4 trials**

Possible values of X are:

$$[ X \in {0, 1, 2, 3, 4} ]$$

---

### Using Multiplication and Addition Rules

Because trials are **independent**, we use:

* **Multiplication rule** → probability of a specific sequence
* **Addition rule (mutually exclusive outcomes)** → sum of probabilities of all valid sequences

---

### Probability of Each Value of X

#### 1. Probability that X = 4 (all red balls)

Only one sequence is possible: RRRR

$$[
P(X = 4) = p \times p \times p \times p = p^4
]$$

---

#### 2. Probability that X = 3 (three red, one blue)

* Number of such sequences = 4
  (BRRR, RBRR, RRBR, RRRB)
* Probability of each sequence = ( p^3 (1 - p) )

$$[
P(X = 3) = 4 \cdot p^3 (1 - p)
]$$

---

#### 3. Probability that X = 2 (two red, two blue)

* Number of such sequences = 6
* Probability of each sequence = ( p^2 (1 - p)^2 )

$$[
P(X = 2) = 6 \cdot p^2 (1 - p)^2
]$$

---

#### 4. Probability that X = 1 (one red, three blue)

* Number of such sequences = 4
* Probability of each sequence = ( p (1 - p)^3 )

$$[
P(X = 1) = 4 \cdot p (1 - p)^3
]$$

---

#### 5. Probability that X = 0 (all blue balls)

Only one sequence is possible: BBBB

$$[
P(X = 0) = (1 - p)^4
]$$

---

### Probability Distribution of X

| X (Number of red balls) | Probability P(X)  |
| ----------------------- |-------------------|
| 0                       | $((1 - p)^4)$     |
| 1                       | $(4p(1 - p)^3)$   |
| 2                       | $(6p^2(1 - p)^2)$ |
| 3                       | $(4p^3(1 - p))$   |
| 4                       | $(p^4)$           |

This table represents the **probability distribution** of the random variable X.

---

### Key Observations

* All probabilities are **non-negative**
* The probabilities **add up to 1**
* The coefficients (1, 4, 6, 4, 1) come from **combinations**
* The distribution depends only on:

  * Number of trials (n = 4)
  * Probability of success (p)

---

### Why This Is Important

This distribution appears **very frequently** in real-world problems such as:

* Number of defective items in a batch
* Number of heads in coin tosses
* Pass/fail outcomes
* Yes/no decisions

This specific structure is called the **Binomial Distribution**.

---

### What’s Next

In the next section, we will:

* Generalise this result to **n trials**
* Introduce the **binomial distribution formula**
* Understand its assumptions formally

## Binomial Distribution – Part 1

### Motivation

Earlier, we calculated probabilities for the red–blue ball game **theoretically** and compared them with experimental results. The key advantage of the theoretical approach is that:

* We do **not** need to repeat experiments many times
* Probabilities can be calculated using **pen-and-paper mathematics**
* The same framework can be **generalised** to many similar problems

In this section, we generalise the earlier setup and arrive at the **binomial distribution**.

---

### Experiment Setup (Generalised)

* An experiment consists of **4 independent trials**
* In each trial:

  * Probability of drawing a **red ball** = **p**
  * Probability of drawing a **blue ball** = **1 − p**
* A ball is **replaced** after every draw, so probabilities remain constant

We define a random variable:

> **X = number of red balls drawn in 4 trials**

Possible values of X are:

$$[ X \in {0, 1, 2, 3, 4} ]$$

---

### Using Multiplication and Addition Rules

Because trials are **independent**, we use:

* **Multiplication rule** → probability of a specific sequence
* **Addition rule (mutually exclusive outcomes)** → sum of probabilities of all valid sequences

---

### Probability of Each Value of X

#### 1. Probability that X = 4 (all red balls)

Only one sequence is possible: RRRR

$$[
P(X = 4) = p \times p \times p \times p = p^4
]$$

---

#### 2. Probability that X = 3 (three red, one blue)

* Number of such sequences = 4
  (BRRR, RBRR, RRBR, RRRB)
* Probability of each sequence = ( p^3 (1 - p) )

$$[
P(X = 3) = 4 \cdot p^3 (1 - p)
]$$

---

#### 3. Probability that X = 2 (two red, two blue)

* Number of such sequences = 6
* Probability of each sequence = ( p^2 (1 - p)^2 )

$$[
P(X = 2) = 6 \cdot p^2 (1 - p)^2
]$$

---

#### 4. Probability that X = 1 (one red, three blue)

* Number of such sequences = 4
* Probability of each sequence = ( p (1 - p)^3 )

$$[
P(X = 1) = 4 \cdot p (1 - p)^3
]$$

---

#### 5. Probability that X = 0 (all blue balls)

Only one sequence is possible: BBBB

$$[
P(X = 0) = (1 - p)^4
]$$

---

### Probability Distribution of X

| X (Number of red balls) | Probability P(X) |
| ----------------------- | ---------------- |
| 0                       | ((1 - p)^4)      |
| 1                       | (4p(1 - p)^3)    |
| 2                       | (6p^2(1 - p)^2)  |
| 3                       | (4p^3(1 - p))    |
| 4                       | (p^4)            |

This table represents the **probability distribution** of the random variable X.

---

### Key Observations

* All probabilities are **non-negative**
* The probabilities **add up to 1**
* The coefficients (1, 4, 6, 4, 1) come from **combinations**
* The distribution depends only on:

  * Number of trials (n = 4)
  * Probability of success (p)

---

### Why This Is Important

This distribution appears **very frequently** in real-world problems such as:

* Number of defective items in a batch
* Number of heads in coin tosses
* Pass/fail outcomes
* Yes/no decisions

This specific structure is called the **Binomial Distribution**.

---

### What’s Next

In the next section, we will:

* Generalise this result to **n trials**
* Introduce the **binomial distribution formula**
* Understand its assumptions formally

## Introduction to Binomial Probability Distribution (General Case)

So far, we considered a fixed number of trials (4 draws). Let us now **generalise** this setup.

### Generalised Experiment Setup

* Total number of trials: **n**
* Probability of success (e.g., drawing a red ball) in each trial: **p**, where 0 ≤ p ≤ 1
* Probability of failure (e.g., drawing a blue ball): **1 − p**
* Random variable **X**: number of successes (red balls) obtained in n trials

Our goal is to find:

> **P(X = r)** = probability of getting exactly **r successes** in **n trials**

### Probability of One Specific Outcome

To get **r red balls** and **(n − r) blue balls** in a *specific order*, the probability is:

$$pʳ × (1 − p)ⁿ⁻ʳ$$

This comes from the multiplication rule, since trials are independent.

### Number of Possible Arrangements

The r successes and (n − r) failures can occur in many different orders.

From combinatorics, the number of such arrangements is:

**$$ⁿCʳ$$** $$("n choose r")$$

### Binomial Probability Formula

Combining both ideas:

> **$$P(X = r) = ⁿCʳ · pʳ · (1 − p)ⁿ⁻ʳ$$**

This formula gives the probability that the random variable X takes the value r.

### Binomial Probability Distribution Table (Conceptual)

| r (number of successes) | P(X = r)                  |
| ----------------------- |---------------------------|
| 0                       | $$ⁿC⁰ · p⁰ · (1 − p)ⁿ$$   |
| 1                       | $$ⁿC¹ · p¹ · (1 − p)ⁿ⁻¹$$ |
| 2                       | $$ⁿC² · p² · (1 − p)ⁿ⁻²$$ |
| ⋮                       | ⋮                         |
| n                       | $$ⁿCⁿ · pⁿ · (1 − p)⁰$$   |

This complete table is called the **Binomial Probability Distribution**.

### Conditions for Applying Binomial Distribution

The binomial distribution can be used **only if all of the following conditions are satisfied**:

1. The total number of trials **n is fixed**
2. Each trial has **only two possible outcomes** (success/failure)
3. The probability of success **p remains constant** across trials
4. Trials are **independent** of each other

### Final Formula (Standard Form)

**$P(X = r) = ⁿCʳ · pʳ · (1 − p)ⁿ⁻ʳ$**

Where:

* n = number of trials
* r = number of successes
* p = probability of success in one trial

The binomial distribution is one of the most commonly used **discrete probability distributions**, especially in situations involving repeated binary outcomes.

## Applicability of the Binomial Distribution

In the previous section, we listed the conditions that must be satisfied for a **binomial distribution** to be applicable. Let us revisit these conditions and understand them through examples.

### Conditions for Binomial Distribution

A binomial distribution can be used when all of the following conditions are satisfied:

1. **Fixed number of trials (n):** The experiment consists of a fixed number of repetitions.
2. **Binary outcomes:** Each trial has only two possible outcomes (success/failure, yes/no, head/tail).
3. **Constant probability (p):** The probability of success remains the same for every trial.
4. **Independence of trials:** The outcome of one trial does not affect the outcomes of other trials.

---

## Examples

### Situations Where Binomial Distribution Is Applicable

| Experiment                                                                | Reason                                                                               |
| ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |
| Tossing a coin 20 times to count the number of tails                      | Fixed number of trials, two outcomes, constant probability (0.5), independent trials |
| Asking 200 randomly selected people whether they are older than 21 or not | Fixed number of trials, yes/no outcome, constant probability                         |
| Drawing a ball from a bag and replacing it each time, repeated 4 times    | Probability remains constant due to replacement                                      |

---

### Situations Where Binomial Distribution Is **Not** Applicable

| Experiment                                 | Reason                              |
| ------------------------------------------ | ----------------------------------- |
| Tossing a coin until the first head occurs | Number of trials is not fixed       |
| Asking 200 people their exact age          | More than two possible outcomes     |
| Drawing balls without replacement          | Probability changes after each draw |

---

## Explanation of Non-Applicable Cases

### 1. Variable Number of Trials

If a coin is tossed until the first head appears, the total number of trials is not fixed. Although we can calculate the probability of getting a head on the 1st, 2nd, or 3rd trial, the **binomial distribution cannot be used** because the number of trials is not predetermined. This type of experiment follows a **geometric distribution**, not a binomial distribution.

---

### 2. More Than Two Outcomes

In the case where people are asked their exact age, there are multiple possible outcomes rather than just two. Similarly, drawing balls of many different colours from a bag results in more than two outcomes. Since binomial distribution only applies to **binary outcomes**, it is not suitable here.

---

### 3. Changing Probabilities (No Replacement)

Consider drawing red balls from a bag **without replacement**. Suppose the bag initially contains 3 red and 2 blue balls:

* Probability of drawing a red ball on the first trial = 3/5
* Probability of drawing a red ball on the second trial = 2/4 (if a red ball was drawn earlier)

The probability changes from trial to trial. Hence, the trials are **not independent**, and the binomial distribution cannot be applied.

For example, the probability of drawing the sequence Red–Red–Red–Blue would be:

3/5 × 2/4 × 1/3 × 2/2

This differs from the binomial assumption, where the probability would incorrectly be treated as:

3/5 × 3/5 × 3/5 × 2/2

---

## Key Takeaway

The binomial distribution is applicable only when we have:

* A **fixed number of trials**,
* Each trial answering a **yes/no question**,
* A **constant probability** of success,
* And **independent trials**.

---

## Binomial Distribution: Worked Example

### Example: Even-Numbered License Plates

Suppose you are walking on a street and observe the next **10 cars**. You want to find the probability that **exactly 5** of them have even-numbered license plates.

* Number of trials (n) = 10
* Two possible outcomes: even or odd license plate
* Probability of success (even-numbered plate), p = 0.5
* Probability of failure, q = 0.5

Using the binomial probability formula:

P(X = 5) = ¹⁰C₅ × (0.5)⁵ × (0.5)⁵

This evaluates to approximately **0.246**, or **24.6%**.

---

## Other Discrete Probability Distributions

Not all discrete random variables follow a binomial distribution. One commonly encountered alternative is the **uniform distribution**.

### Uniform Distribution Example: Dice Roll

When a fair die is rolled:

* Possible outcomes: {1, 2, 3, 4, 5, 6}
* Each outcome has an equal probability of **1/6**

Since all outcomes are equally likely, the random variable follows a **uniform distribution**, not a binomial distribution.

---

## Summary

* Binomial distribution applies to repeated, independent, yes/no experiments with constant probability.
* Experiments with variable trials, multiple outcomes, or changing probabilities require other probability distributions.
* Understanding the conditions is crucial before choosing the appropriate probability model.

## Cumulative Probability Distributions

In earlier examples, we focused on finding the probability that a random variable takes an **exact value**, such as P(X = 4). However, in many real-life situations, we are more interested in probabilities of the form **less than**, **less than or equal to**, or **between two values**. These probabilities are captured using **cumulative probability distributions**.

---

## Motivation: Why Cumulative Probability?

Consider the following situations:

* A casino wants to know the probability that a player gets **fewer than 3 red balls**, since that means the player loses and the house profits.
* A company wants to know how many employees can reach work in **less than 40 minutes**.

In such cases, knowing probabilities for exact values is not sufficient. We need probabilities for **ranges of values**.

---

## Example: Red and Blue Balls Game

Recall the red and blue balls game discussed earlier:

* A player **wins** if they draw **exactly 4 red balls**.
* A player **loses** if they draw **3 or fewer red balls**.

Let the random variable **X** represent the number of red balls drawn.

The probability distribution for X was calculated earlier as follows:

| X (Number of red balls) | P(X)   |
| ----------------------- | ------ |
| 0                       | 0.0256 |
| 1                       | 0.1536 |
| 2                       | 0.3456 |
| 3                       | 0.3456 |
| 4                       | 0.1296 |

---

## Calculating Probability of Losing

The player loses when **X ≤ 3**. Therefore,

P(Player loses) = P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3)

Instead of repeatedly adding probabilities, we use the concept of **cumulative probability**.

---

## Definition: Cumulative Distribution Function (CDF)

The **cumulative distribution function (CDF)** of a discrete random variable X is defined as:

F(x) = P(X ≤ x)

It gives the probability that the random variable takes a value **less than or equal to x**.

---

## Cumulative Probability Table (Red Balls Example)

| x | P(X = x) | F(x) = P(X ≤ x) |
| - | -------- | --------------- |
| 0 | 0.0256   | 0.0256          |
| 1 | 0.1536   | 0.1792          |
| 2 | 0.3456   | 0.5248          |
| 3 | 0.3456   | 0.8704          |
| 4 | 0.1296   | 1.0000          |

---

## Key Observations About CDF

1. **F(x) is non-decreasing**: It never decreases as x increases.
2. **0 ≤ F(x) ≤ 1** for all values of x.
3. **F(max value of X) = 1**.
4. For the smallest possible value of X, F(x) equals P(X = x).

---

## Using Cumulative Probability

### Example 1: Probability of Losing

P(X ≤ 3) = F(3) = **0.8704**

This is the probability that the player loses the game.

---

## Example 2: Employee Weight Distribution

Suppose the cumulative probability distribution of employee weights is given. Let **X** represent the weight (in kg) of an employee.

We are asked to find the probability that an employee’s weight lies **between 60 kg and 65 kg**.

This can be written as:

P(60 ≤ X ≤ 65)

Using cumulative probabilities:

P(60 ≤ X ≤ 65) = P(X ≤ 65) − P(X ≤ 60)

From the table:

* F(65) = 0.467
* F(60) = 0.367

Therefore,

P(60 ≤ X ≤ 65) = 0.467 − 0.367 = **0.10**

---

## General Rule for Cumulative Probability

For any discrete random variable X:

P(a ≤ X ≤ b) = F(b) − F(a − 1)

(For continuous variables, this becomes P(a ≤ X ≤ b) = F(b) − F(a))

---

## Summary

* Cumulative probability answers **range-based probability questions**.
* The cumulative distribution function F(x) = P(X ≤ x).
* CDFs simplify probability calculations for events like *less than*, *at most*, and *between*.
* The final value of any CDF is always **1**.

Cumulative probability distributions form the foundation for both **discrete** and **continuous** probability models used extensively in statistics and data science.
