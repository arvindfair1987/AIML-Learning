
# NumPy Linear Algebra (`numpy.linalg`) â€” Examples with Output & Explanation

```python
import numpy as np
```

---

## 1. `np.linalg.solve(A, b)`

Solves the linear system **Ax = b**

```python
A = np.array([[3, 1],
              [1, 2]])
b = np.array([9, 8])

x = np.linalg.solve(A, b)
print(x)
```

**Output**

```
[2. 3.]
```

**Explanation**
The solution vector `[2, 3]` satisfies:

```
3x + y = 9
x + 2y = 8
```

---

## 2. `np.linalg.inv(A)`

Computes the **inverse of a matrix**

```python
A = np.array([[1, 2],
              [3, 4]])

print(np.linalg.inv(A))
```

**Output**

```
[[-2.   1. ]
 [ 1.5 -0.5]]
```

**Explanation**
Multiplying `A @ inv(A)` gives the **identity matrix**.

---

## 3. `np.linalg.pinv(A)`

Moore-Penrose **pseudo-inverse** (works even if inverse doesnâ€™t exist)

```python
A = np.array([[1, 2],
              [2, 4]])

print(np.linalg.pinv(A))
```

**Output**

```
[[0.04 0.08]
 [0.08 0.16]]
```

**Explanation**
Used when matrices are **singular or non-square**.

---

## 4. `np.linalg.lstsq(A, b)`

Solves **least squares problems**

```python
A = np.array([[1, 1],
              [1, 2],
              [1, 3]])
b = np.array([1, 2, 2])

x, residuals, _, _ = np.linalg.lstsq(A, b, rcond=None)
print(x)
```

**Output**

```
[0.66666667 0.5]
```

**Explanation**
Finds the **best-fit line** `y = 0.67 + 0.5x`.

---

## 5. `np.linalg.det(A)`

Computes the **determinant**

```python
A = np.array([[1, 2],
              [3, 4]])

print(np.linalg.det(A))
```

**Output**

```
-2.0
```

**Explanation**
A non-zero determinant means the matrix is **invertible**.

---

## 6. `np.linalg.norm(x)`

Computes the **vector or matrix norm**

```python
v = np.array([3, 4])
print(np.linalg.norm(v))
```

**Output**

```
5.0
```

**Explanation**
This is the **Euclidean length**: âˆš(3Â² + 4Â²).

---

## 7. `np.linalg.matrix_rank(A)`

Finds the **rank** of a matrix

```python
A = np.array([[1, 2],
              [2, 4]])

print(np.linalg.matrix_rank(A))
```

**Output**

```
1
```

**Explanation**
Rows are linearly dependent â†’ rank is 1.

---

## 8. `np.linalg.slogdet(A)`

Returns **sign and log(|determinant|)**

```python
A = np.array([[1, 0],
              [0, -2]])

sign, logdet = np.linalg.slogdet(A)
print(sign, logdet)
```

**Output**

```
-1.0 0.6931471805599453
```

**Explanation**
Used for **numerical stability** with large determinants.

---

## 9. `np.trace(A)`

Sum of diagonal elements

```python
A = np.array([[1, 2],
              [3, 4]])

print(np.trace(A))
```

**Output**

```
5
```

**Explanation**
Trace = 1 + 4.

---

## 10. `np.linalg.eig(A)`

Eigenvalues and eigenvectors

```python
A = np.array([[2, 0],
              [0, 3]])

values, vectors = np.linalg.eig(A)
print(values)
print(vectors)
```

**Output**

```
[2. 3.]
[[1. 0.]
 [0. 1.]]
```

**Explanation**
Eigenvalues represent **scaling factors** of eigenvectors.

---

## 11. `np.linalg.eigvals(A)`

Eigenvalues only

```python
A = np.array([[2, 1],
              [1, 2]])

print(np.linalg.eigvals(A))
```

**Output**

```
[3. 1.]
```

**Explanation**
Same as `eig()` but faster if vectors arenâ€™t needed.

---

## 12. `np.linalg.eigh(A)`

Eigenvalues of **symmetric matrices**

```python
A = np.array([[2, 1],
              [1, 2]])

vals, vecs = np.linalg.eigh(A)
print(vals)
```

**Output**

```
[1. 3.]
```

**Explanation**
More stable and guarantees **real eigenvalues**.

---

## 13. `np.linalg.cond(A)`

Condition number

```python
A = np.array([[1, 2],
              [3, 4]])

print(np.linalg.cond(A))
```

**Output**

```
14.933034...
```

**Explanation**
Large values mean the matrix is **numerically unstable**.

---

## 14. `np.linalg.qr(A)`

QR decomposition

```python
A = np.array([[1, 1],
              [1, 0]])

Q, R = np.linalg.qr(A)
print(Q)
print(R)
```

**Output**

```
Q = orthogonal matrix
R = upper triangular matrix
```

**Explanation**
Used in **least squares** and numerical algorithms.

---

## 15. `np.linalg.svd(A)`

Singular Value Decomposition

```python
A = np.array([[1, 2],
              [3, 4]])

U, S, Vt = np.linalg.svd(A)
print(S)
```

**Output**

```
[5.4649857  0.36596619]
```

**Explanation**
Breaks matrix into rotations + scaling. Used in **PCA**, compression.

---

## 16. `np.linalg.cholesky(A)`

Cholesky decomposition (SPD matrices only)

```python
A = np.array([[4, 2],
              [2, 3]])

print(np.linalg.cholesky(A))
```

**Output**

```
[[2.         0.        ]
 [1.         1.41421356]]
```

**Explanation**
Fast decomposition for **positive-definite matrices**.

---

## âœ… Summary

`numpy.linalg` provides tools for:

* Solving equations
* Matrix decomposition
* Eigen analysis
* Numerical stability checks

Perfect for **data science, ML, physics, and engineering** ðŸš€

---
Got it â€” this looks like **course material**, and what you likely need is a **clear, organized explanation** of the key ideas (inverse, rank, column space, null space, least squares, and eigenvectors) without the clutter and repeated â€œEquationâ€ placeholders.

Below is a **clean, student-friendly explanation**, concept by concept, tying together the **algebra + geometry intuition**.

---

## 1. Inverse of a Matrix

### What does an inverse mean?

If a matrix (A) represents a transformation, then its inverse (A^{-1}) **undoes** that transformation.

$[
A^{-1}(A\mathbf{x}) = \mathbf{x}
]$

### When does an inverse exist?

A matrix is **invertible** if and only if:

* It is **square**
* Its **determinant is non-zero**
* It does **not collapse (squish)** space into a lower dimension

### Non-invertible (Singular) matrices

A matrix is **singular** if:

* $(\det(A) = 0)$
* The inverse does **not** exist

#### Geometric intuition

* Normal case: 2D â†’ 2D, 3D â†’ 3D
* Singular case:

  * 2D â†’ line
  * 3D â†’ plane or line

Once space is squished, information is lost â†’ cannot reverse the transformation.

---

## 2. Rank of a Matrix

### Definition

The **rank** of a matrix is:

> The **dimension of the output space** of the transformation

Equivalently:

* Number of **linearly independent columns**
* Dimension of the **column space**

### Example

If two columns are multiples of each other:

```text
Rank = 1
```

This means:

* A 2D input is squished onto a **line**

---

## 3. Column Space

### Definition

The **column space** of a matrix is:

> The set of all vectors that can be produced by the matrix

Mathematically:
$[
\text{Col}(A) = \text{span of the columns of } A
]$

### Key idea

* Column space tells you **where outputs can land**
* Rank = dimension of column space

If a vector (b) is **not** in the column space â†’
âŒ the system (Ax = b) has **no solution**

---

## 4. Null Space

### Definition

The **null space** is:

> The set of all vectors that map to the zero vector

$[
\text{Null}(A) = {\mathbf{x} : A\mathbf{x} = \mathbf{0}}
]$

### Geometric meaning

* These vectors get **completely destroyed** by the transformation
* Non-trivial null space â‡’ matrix is **not invertible**

---

## 5. Why Some Systems Have No Solution

If:

* Input space = 3D
* Rank = 2

Then:

* The transformation squishes 3D â†’ 2D plane
* Only vectors **on that plane** can be reached

If (b) is not on that plane:
$[
Ax = b \quad \text{has no solution}
]$

---

## 6. Least Squares Approximation

### Why do we need it?

In real-world problems:

* Systems are often **overdetermined**
* Exact solutions often **do not exist**

### Goal

Find (x) such that:
$[
Ax \approx b
]$
as closely as possible.

### Least squares solution

$[
x = (A^T A)^{-1} A^T b
]$

### Geometric intuition

* Project (b) onto the **column space of (A)**
* Choose the closest reachable point

This is the foundation of:

* **Linear regression**
* **Machine learning optimization**

---

## 7. Eigenvectors and Eigenvalues (Introduction)

### Definition

A **non-zero vector** (\mathbf{v}) is an eigenvector of (A) if:
$$[
A\mathbf{v} = \lambda \mathbf{v}
]$$

* $(\mathbf{v})$ â†’ eigenvector
* $(\lambda)$ â†’ eigenvalue

### Key idea

Eigenvectors:

* **Do not change direction**
* Only get **scaled**

Eigenvalues:

* Tell you **how much scaling occurs**
* Negative â†’ direction reverses

---

## 8. Why Eigenvectors Matter

Eigenvectors:

* Reveal the **natural axes** of a transformation
* Are used in:

  * PCA (Principal Component Analysis)
  * Data compression
  * Stability analysis
  * Physics & engineering

---

## Big Picture Summary

| Concept         | Meaning                   |
| --------------- | ------------------------- |
| Inverse         | Undoing a transformation  |
| Rank            | Output dimensionality     |
| Column Space    | All reachable outputs     |
| Null Space      | Inputs that vanish        |
| Singular Matrix | Loses dimensionality      |
| Least Squares   | Best approximate solution |
| Eigenvector     | Direction preserved       |
| Eigenvalue      | Scaling factor            |



---

# Finding Eigenvalues and Eigenvectors (Clean Notes)

## 1. What is an eigenvalueâ€“eigenvector pair?

For a square matrix (A):

$[
A\mathbf{x} = \lambda \mathbf{x}
]$

* $(\mathbf{x} \neq \mathbf{0})$ is an **eigenvector**
* $(\lambda)$ is the corresponding **eigenvalue**

This equation means:

> Applying transformation (A) to vector (\mathbf{x}) only **scales** it, without changing its direction.

---

## 2. Rearranging the eigenvalue equation

$[
A\mathbf{x} = \lambda \mathbf{x}
]$

Bring everything to one side:

$[
A\mathbf{x} - \lambda \mathbf{x} = 0
]$

Factor out (\mathbf{x}):

$[
(A - \lambda I)\mathbf{x} = 0
]$

Here:

* (I) is the **identity matrix**
* (A - \lambda I) is a matrix depending on (\lambda)

---

## 3. Why do we take the determinant?

The equation
$[
(A - \lambda I)\mathbf{x} = 0
]$
has a **non-zero solution** only if the matrix (A - \lambda I) is **singular**.

That happens when:
$[
\boxed{\det(A - \lambda I) = 0}
]$

This equation is called the **characteristic equation**.

---

## 4. Finding eigenvalues

### Step 1: Form (A - \lambda I)

If
$[
A =
\begin{bmatrix}
3 & -5 \\
-6 & 4
\end{bmatrix}
]$

then
$[
A - \lambda I =
\begin{bmatrix}
3-\lambda & -5 \\
-6 & 4-\lambda
\end{bmatrix}
]$

---

### Step 2: Compute the determinant

$[
\det(A - \lambda I)
=
(3-\lambda)(4-\lambda) - (-5)(-6)
]$

$[
= (12 - 7\lambda + \lambda^2) - 30
]$

$[
= \lambda^2 - 7\lambda - 18
]$

---

### Step 3: Solve the characteristic equation

$[
\lambda^2 - 7\lambda - 18 = 0
]$

Factor:
$[
(\lambda - 9)(\lambda + 2) = 0
]$

---

### âœ… Eigenvalues

$[
\boxed{\lambda = 9 \quad \text{and} \quad \lambda = -2}
]$

---

## 5. Finding eigenvectors

For each eigenvalue (\lambda), solve:
$[
(A - \lambda I)\mathbf{x} = 0
]$

---

### Eigenvector for (\lambda = 9)

$[
A - 9I =
\begin{bmatrix}
-6 & -5 \\
-6 & -5
\end{bmatrix}
]$

Solve:
$[
-6x - 5y = 0
\Rightarrow x = -\frac{5}{6}y
]$

One eigenvector is:
$[
\boxed{
\begin{bmatrix}
-5 \\
6
\end{bmatrix}
}
]$

---

### Eigenvector for (\lambda = -2)

$[
A + 2I =
\begin{bmatrix}
5 & -5 \\
-6 & 6
\end{bmatrix}
]$

Solve:
$[
5x - 5y = 0
\Rightarrow x = y
]$

One eigenvector is:
$[
\boxed{
\begin{bmatrix}
1 \\
1
\end{bmatrix}
}
]$

---

## 6. Final Summary (Exam-Perfect)

**Steps to find eigenvalues and eigenvectors:**

1. Solve
   $[
   \det(A - \lambda I) = 0
   ]$
   â†’ gives **eigenvalues**
2. For each eigenvalue (\lambda), solve
   $[
   (A - \lambda I)\mathbf{x} = 0
   ]$
   â†’ gives **eigenvectors**

---

---

# Finding Eigenvectors (Clean & Correct Notes)

We already know that eigenvalues are found by solving
$[
\det(A-\lambda I)=0
]$

Once eigenvalues are known, **eigenvectors are found separately for each eigenvalue**.

---

## Given Matrix

$[
A =
\begin{bmatrix}
3 & -5 \\
-6 & 4
\end{bmatrix}
]$

The eigenvalues are:
$[
\boxed{\lambda = 9 \quad \text{and} \quad \lambda = -2}
]$

---

## General Rule to Find Eigenvectors

For each eigenvalue (\lambda), solve:
$[
(A-\lambda I)\mathbf{x} = 0
]$

Let
$[
\mathbf{x} =
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
]$

---

## Eigenvector for (\lambda = 9)

### Step 1: Compute (A - 9I)

$[
A - 9I =
\begin{bmatrix}
3-9 & -5 \\
-6 & 4-9
\end{bmatrix}
=
\begin{bmatrix}
-6 & -5 \\
-6 & -5
\end{bmatrix}
]$

---

### Step 2: Solve ((A - 9I)\mathbf{x} = 0)

$[
-6x_1 - 5x_2 = 0
]$

$[
6x_1 + 5x_2 = 0
\Rightarrow x_1 = -\frac{5}{6}x_2
]$

Choose $(x_2 = 6), then (x_1 = -5)$

---

### âœ… Eigenvector for (\lambda = 9)

$[
\boxed{
\begin{bmatrix}
-5 \\
6
\end{bmatrix}
}
]$

(Any non-zero scalar multiple is also valid.)

---

## Eigenvector for (\lambda = -2)

### Step 1: Compute (A + 2I)

$[
A + 2I =
\begin{bmatrix}
5 & -5 \\
-6 & 6
\end{bmatrix}
]$

---

### Step 2: Solve ((A + 2I)\mathbf{x} = 0)

$[
5x_1 - 5x_2 = 0
\Rightarrow x_1 = x_2
]$

---

### âœ… Eigenvector for (\lambda = -2)

$[
\boxed{
\begin{bmatrix}
1 \\
1
\end{bmatrix}
}
]$

---

## Verification (Optional but Powerful)

Eigenvectors must satisfy:
$[
A\mathbf{x} = \lambda \mathbf{x}
]$

### For (\lambda = 9):

$[
A
\begin{bmatrix}
-5 \\
6
\end{bmatrix}
=
\begin{bmatrix}
-45 \\
54
\end{bmatrix}
=
9
\begin{bmatrix}
-5 \\
6
\end{bmatrix}
]$

âœ” Verified

---

### For (\lambda = -2):

$[
A
\begin{bmatrix}
1 \\
1
\end{bmatrix}
=
\begin{bmatrix}
-2 \\
-2
\end{bmatrix}
=
-2
\begin{bmatrix}
1 \\
1
\end{bmatrix}
]$

âœ” Verified

---

## Example: Diagonal Matrix

$[
A =
\begin{bmatrix}
-1 & 0 \\
0 & 1
\end{bmatrix}
]$

### Eigenvalues:

$[
\lambda = -1,; 1
]$

---

### Eigenvectors

* For $(\lambda = -1)$:
  All vectors on the **x-axis**
  $[
  \boxed{
  \begin{bmatrix}
  1 \\
  0
  \end{bmatrix}
  }
  ]$

* For $(\lambda = 1)$:
  All vectors on the **y-axis**
  $[
  \boxed{
  \begin{bmatrix}
  0 \\
  1
  \end{bmatrix}
  }
  ]$

---

## Important Notes (Exam Gold)

* Eigenvectors are **not unique** (any scalar multiple works)
* Conventionally, we report **simple or normalized vectors**
* Each eigenvalue has its **own eigenspace**
* If (\det(A-\lambda I) \neq 0), no eigenvector exists

---

## One-Line Summary

> **Eigenvalues tell you how much a direction scales; eigenvectors tell you which directions stay fixed.**

