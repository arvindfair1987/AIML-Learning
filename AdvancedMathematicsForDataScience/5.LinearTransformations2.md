# Linear Algebra for Data Science

## Vectors, Linear Transformations, Eigenvalues

---

## Session Overview

In this session, we cover:

1. Linear transformations
2. Changing bases and transformations in different bases
3. Eigenvalues and eigenvectors
4. Finding eigenvalues and eigenvectors
5. Eigendecomposition of a matrix

---

## 1. Introduction to Vectors in Data Science

Vectors are the **language of data**.

In data science and machine learning:

* Customer behavior
* Image pixels
* Word embeddings (NLP)

All are represented **numerically as vectors**.

Vectors allow us to:

* Measure distance
* Compute similarity
* Transform data

Understanding vectors builds the foundation for:

* Machine learning
* Neural networks
* Optimization algorithms

---

## 2. Vector Fundamentals

### What is a Vector?

A **vector** is a quantity with:

* **Magnitude** (length)
* **Direction**

A **scalar** has only magnitude.

---

## 3. One-Dimensional Vectors

Example: Temperature readings over time

| Point | Temperature |
| ----- | ----------- |
| A     | 30          |
| B     | 35          |
| C     | -40         |

These can be represented on a **number line**.

* Vector starts at 0 (origin)
* Ends at the value
* Length = magnitude
* Direction = positive or negative axis

Example:

* (30): length 30, positive direction
* (-40): length 40, negative direction

In **1D**, direction is restricted to a single axis.

---

## 4. Two-Dimensional Vectors

When data has **two attributes**, vectors live in 2D space.

Example:

* Latitude
* Longitude

Each vector needs:

* x-axis
* y-axis

A vector is drawn by:

1. Starting at origin
2. Moving along x-axis
3. Then moving along y-axis

---

## 5. Three-Dimensional Vectors

Used when data has **three features**.

Examples:

* Latitude
* Longitude
* Altitude

Now vectors exist in **3D space**.

---

## 6. Introduction to Linear Transformations

### What is a Transformation?

A **transformation** is simply a **function**:

* Takes an input
* Produces an output

Example (numbers):
$[
T(x) = 2x
]$

Input: 4
Output: 8

---

### Transformations on Vectors

A transformation can also take a **vector as input** and return a **vector as output**.

Example:
$[
T(\mathbf{x}) = 2\mathbf{x}
]$

Every element of the vector is doubled.

---

## 7. Transformations in 2D Space

When a transformation is applied to:

* Every point
* Every vector

The entire space changes shape.

But **not all transformations are linear**.

---

## 8. Linear Transformations

A transformation is **linear** if it satisfies:

### Geometric Conditions

1. A line remains a line
2. The origin remains fixed
3. Grid lines remain parallel and equally spaced

---

### Algebraic Conditions (Important)

A transformation (T) is linear if:

1. **Additivity**
   $[
   T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})
   ]$

2. **Homogeneity (Scaling)**
   $[
   T(c\mathbf{u}) = cT(\mathbf{u})
   ]$

---

## 9. Matrices as Linear Transformations

* Every **matrix transformation** is linear
* Matrices automatically satisfy:

  * Linearity
  * Origin fixed
  * Parallel grid lines

That‚Äôs why matrices are so important in data science.

---

## 10. Linear vs Non-Linear Transformation

Example transformations:

* $(T(\mathbf{x}) = A\mathbf{x})$ ‚Üí **Linear**
* $(T(\mathbf{x}) = A\mathbf{x} + \mathbf{b})$ ‚Üí **Not linear**
  (origin is shifted)

If the **origin is not fixed**, the transformation is **not linear**.

---

## 11. Key Takeaway

* Vectors represent data
* Transformations change vectors
* **Linear transformations preserve structure**
* Matrices are tools to represent linear transformations

Perfect ‚Äî this again looks like **lecture transcript text**. I‚Äôll **clean it up, correct notation, remove repetition, and present it as crisp notes** that you can revise before exams or interviews.

---

# Matrices as Transformations

## Core Idea

A **matrix represents a linear transformation**.

When a matrix multiplies a vector, it **transforms** that vector:

* It may rotate it
* Stretch or shrink it
* Reflect it
* Or even change its dimension

Mathematically:
$[
\mathbf{B} = A\mathbf{X}
]$

Here:

* $(\mathbf{X})$ = input vector
* (A) = transformation matrix
* $(\mathbf{B})$ = output vector

---

## Matrices as Functions

A matrix can be viewed as a **function**:
$[
T(\mathbf{x}) = A\mathbf{x}
]$

* Input: vector $(\mathbf{x})$
* Output: transformed vector $(A\mathbf{x})$

So, **matrix‚Äìvector multiplication = applying a transformation**.

---

## Dimension Change Using Matrices

Matrices allow us to transform vectors **from one dimension to another**.

Example:

* Input vector $(\mathbf{X})$ is in **3D**
* Output vector $(\mathbf{B})$ is in **2D**

This is possible when:

* (A) is a $(2 \times 3)$ matrix
* $(\mathbf{X})$ is a $(3 \times 1)$ vector
* Result $(\mathbf{B})$ is a $(2 \times 1)$ vector

So matrices can **reduce or increase dimensionality**.

---

## Example: Matrix‚ÄìVector Transformation in 2D

### Given

Vector:
$[
\mathbf{U} =
\begin{bmatrix}
1 \\
-1
\end{bmatrix}
]$

Transformation matrix:
$[
T =
\begin{bmatrix}
2 & 1 \\
-1 & 3
\end{bmatrix}
]$

---

### Applying the Transformation

$[
\mathbf{U'} = T\mathbf{U}
]$
=
$
\begin{bmatrix}
2 & 1 \\
-1 & 3
\end{bmatrix}
$
$
\begin{bmatrix}
1 \\
-1
\end{bmatrix}
]$

Compute:
$
[
\mathbf{U'} =
\begin{bmatrix}
2(1) + 1(-1) \\
-1(1) + 3(-1)
\end{bmatrix}
$
$=$
$
\begin{bmatrix}
1 \\
-4
\end{bmatrix}
]$

---

### Interpretation

* Blue vector $(\mathbf{U})$ ‚Üí original vector
* Green vector $(\mathbf{U'})$ ‚Üí transformed vector
* Both vectors lie in **standard 2D coordinate space**
* The matrix **changes the direction and magnitude** of the vector

---

## Key Observations

* Every matrix defines a transformation
* Matrix‚Äìvector multiplication applies that transformation
* Linear transformations:

  * Preserve straight lines
  * Keep the origin fixed
  * Maintain parallel grid lines

---

## Important Conclusion

If:
$$[
T(\mathbf{x}) = A\mathbf{x}
]$$

Then:

* (T) is a **linear transformation**
* Every matrix defines a **linear transformation**
* This idea is the foundation for:

  * Eigenvalues & eigenvectors
  * PCA
  * Neural networks
  * Dimensionality reduction

---
Great question ‚Äî this topic **always clicks** once the properties are written cleanly with correct math notation. I‚Äôll do three things for you:

1. **Clean and rewrite the notes properly**
2. **Fix all missing math notation using `$ ‚Ä¶ $`**
3. **Give a clear example of a NON-linear transformation involving matrices**

---

# Properties of Linear Transformations

## Introduction

We want to determine whether transformations defined using **matrices** are **linear**.

A matrix transformation has the form:
$[
T(\mathbf{x}) = A\mathbf{x}
]$
where:

* $\mathbf{x}$ is a vector
* $A$ is a matrix

To check if $T$ is linear, we verify **two properties**.

---

## Conditions for Linearity

A transformation $T$ is **linear** if it satisfies:

### 1. Additivity (Preserves Vector Addition)

$[
T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})
]$

### 2. Homogeneity (Preserves Scalar Multiplication)

$[
T(c\mathbf{u}) = cT(\mathbf{u})
]$

---

## Example 1: Verifying Additivity

### Given

Vectors:
$[
\mathbf{u} =
\begin{bmatrix}
1 \\
2
\end{bmatrix},
\quad
\mathbf{v} =
\begin{bmatrix}
-1 \\
3
\end{bmatrix}
]$

Matrix:
$[
A =
\begin{bmatrix}
1 & 1 \\
0 & 1
\end{bmatrix}
]$

---

### Step 1: Compute $T(\mathbf{u} + \mathbf{v})$

$[
\mathbf{u} + \mathbf{v} =
\begin{bmatrix}
0 \\
5
\end{bmatrix}
]$

$[
T(\mathbf{u} + \mathbf{v}) =
A
]$
$
\begin{bmatrix}
0 \\
5
\end{bmatrix}
$
=
$
\begin{bmatrix}
5 \\
5
\end{bmatrix}
$

---

### Step 2: Compute $T(\mathbf{u}) + T(\mathbf{v})$

$[
T(\mathbf{u}) =
$
A
$
\begin{bmatrix}
1 \\
2
\end{bmatrix}
$
=
$
\begin{bmatrix}
3 \\
2
\end{bmatrix}
$]

$[
T(\mathbf{v}) =
A]$

$
\begin{bmatrix}
-1 \\
3
\end{bmatrix}
$
=
$
\begin{bmatrix}
2 \\
3
\end{bmatrix}
$

$[
T(\mathbf{u}) + T(\mathbf{v}) =
]$
$[
\begin{bmatrix}
5 \\
5
\end{bmatrix}
]$

---

### Conclusion

$[
T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})
]$

‚úÖ **Additivity holds**

---

## Example 2: Verifying Scalar Multiplication

### Given

Vector:
$[
\mathbf{u} =
\begin{bmatrix}
1 \\
2
\end{bmatrix}
]$

Scalar:
$[
c = 2
]$

---

### Step 1: Compute $T(c\mathbf{u})$

$[
c\mathbf{u} =
\begin{bmatrix}
2 \\
4
\end{bmatrix}
]$

$[
T(c\mathbf{u}) =
A
\begin{bmatrix}
2 \\
4
\end{bmatrix}
]$
=
$[
\begin{bmatrix}
6 \\
4
\end{bmatrix}
]$

---

### Step 2: Compute $cT(\mathbf{u})$

$[
T(\mathbf{u}) =
\begin{bmatrix}
3 \\
2
\end{bmatrix}
]$

$[
cT(\mathbf{u}) =
\begin{bmatrix}
6 \\
4
\end{bmatrix}
]$

---

### Conclusion

$[
T(c\mathbf{u}) = cT(\mathbf{u})
]$

‚úÖ **Homogeneity holds**

---

## Final Conclusion for Matrix Transformations

Since both properties hold:

$[
T(\mathbf{x}) = A\mathbf{x}
]$

‚úîÔ∏è **Every matrix defines a linear transformation**

---

# Example of a NON-Linear Transformation (Using Matrices)

Now the important contrast üëá

## Affine Transformation (NOT Linear)

Consider:
$[
T(\mathbf{x}) = A\mathbf{x} + \mathbf{b}
]$

where:
$[
A =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix},
\quad
\mathbf{b} =
\begin{bmatrix}
2 \\
3
\end{bmatrix}
]$

---

### Check the Origin

$[
T(\mathbf{0}) = A\mathbf{0} + \mathbf{b} = \mathbf{b} \neq \mathbf{0}
]$

‚ùå **Origin is NOT fixed**

---

### Check Additivity

$[
T(\mathbf{u} + \mathbf{v}) = A(\mathbf{u} + \mathbf{v}) + \mathbf{b}
]$

$[
T(\mathbf{u}) + T(\mathbf{v}) = A\mathbf{u} + A\mathbf{v} + 2\mathbf{b}
]$

$[
T(\mathbf{u} + \mathbf{v}) \neq T(\mathbf{u}) + T(\mathbf{v})
]$

‚ùå **Additivity fails**

---

### Conclusion

$[
T(\mathbf{x}) = A\mathbf{x} + \mathbf{b}
]$

‚ùå **NOT a linear transformation**

‚úîÔ∏è This is called an **affine transformation** (translation + linear part)

---

## Summary Table

| Transformation                             | Linear? | Reason                    |
| ------------------------------------------ | ------- | ------------------------- |
| $T(\mathbf{x}) = A\mathbf{x}$              | ‚úÖ Yes   | Satisfies both properties |
| $T(\mathbf{x}) = A\mathbf{x} + \mathbf{b}$ | ‚ùå No    | Origin not fixed          |
| Rotation / Reflection                      | ‚úÖ Yes   | Pure matrix form          |
| Translation                                | ‚ùå No    | Shifts origin             |

---


# Types of Linear Transformations (2D)

Linear transformations in 2D space can be represented using **$2 \times 2$ matrices**.
They transform vectors while preserving:

* straight lines
* parallelism
* the origin

---

## 1. Reflection

### Reflection about the **Y-axis**

Reflection across the Y-axis changes the sign of the **x-coordinate**.

#### Transformation matrix

$[
R_y =
\begin{bmatrix}
-1 & 0 \\
0 & 1
\end{bmatrix}
]$

#### Example

Given vector:
$[
\mathbf{v} =
\begin{bmatrix}
2 \\
2
\end{bmatrix}
]$

Apply reflection:
$[
R_y \mathbf{v} =
\begin{bmatrix}
-1 & 0 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
2 \\
2
\end{bmatrix}
=
\begin{bmatrix}
-2 \\
2
\end{bmatrix}
]$

‚úîÔ∏è Reflected along the Y-axis

---

### Reflection about the **X-axis**

Reflection across the X-axis changes the sign of the **y-coordinate**.

#### Transformation matrix

$[
R_x =
\begin{bmatrix}
1 & 0 \\
0 & -1
\end{bmatrix}
]$

#### Example

$$[
R_x
\begin{bmatrix}
2 \\
2
\end{bmatrix}
=
\begin{bmatrix}
2 \\
-2
\end{bmatrix}
]$$

---

## 2. Rotation

### General Rotation Matrix

A rotation by angle $\theta$ (counter-clockwise):

$[
R(\theta) =
\begin{bmatrix}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{bmatrix}
]$

---

### Rotation by **$90^\circ$ Counter-Clockwise**

$[
\cos 90^\circ = 0, \quad \sin 90^\circ = 1
]$

[
$
R(90^\circ) =
\begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix}
$
]

#### Example

[
$
R(90^\circ)
$
$
\begin{bmatrix}
2 \\
2
\end{bmatrix}
$
=
$
\begin{bmatrix}
-2 \\
2
\end{bmatrix}
$
]

‚úîÔ∏è Vector rotated $90^\circ$ counter-clockwise

---

## 3. Dilation (Scaling)

Dilation changes the **length** of a vector.

### Scaling by factor $k$

[
D =
$
\begin{bmatrix}
k & 0 \\
0 & k
\end{bmatrix}
$
]

---

### Example: Double the vector size ($k = 2$)

[
D =
$
\begin{bmatrix}
2 & 0 \\
0 & 2
\end{bmatrix}
$
]

[
D
$
\begin{bmatrix}
2 \\
2
\end{bmatrix}
$
=
$
\begin{bmatrix}
4 \\
4
\end{bmatrix}
$
]

‚úîÔ∏è Vector length doubled along both axes

---

## 4. Shear Transformation

Shear makes objects **slanted**.

### Shear in the X-direction

[
$
S_x =
\begin{bmatrix}
1 & 1 \\
0 & 1
\end{bmatrix}
$
]

#### Example

[
$
S_x
\begin{bmatrix}
2 \\
2
\end{bmatrix}
$
=
$
\begin{bmatrix}
4 \\
2
\end{bmatrix}
$
]

‚úîÔ∏è X-coordinate changes, Y-coordinate remains same

---

## 5. Identity Transformation

The identity transformation leaves the vector **unchanged**.

### Identity matrix

[
I =
$\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$
]

#### Example


I
$
\begin{bmatrix}
2 \\
2
\end{bmatrix}
$
=
$
\begin{bmatrix}
2 \\
2
\end{bmatrix}
$


‚úîÔ∏è No change in vector

---

## Summary Table of 2D Linear Transformations

| Transformation      | Matrix                                                                              |
| ------------------- |-------------------------------------------------------------------------------------|
| Reflection (Y-axis) | $\begin{bmatrix} -1 & 0 \\ 0 & 1 \end{bmatrix}$                                     |
| Reflection (X-axis) | $\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}$                                     |
| Rotation ($\theta$) | $\begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}$ |
| Dilation            | $\begin{bmatrix} k & 0 \\ 0 & k \end{bmatrix}$                                      |
| Shear (X)           | $\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$                                      |
| Identity            | $\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}$                                      |

---

## Key Takeaway

* All these are **linear transformations**
* All can be written as **matrix‚Äìvector multiplication**
* They preserve structure and keep the **origin fixed**


# Reversing Linear Transformations

So far, we have learned how matrices **transform vectors**.
Now the key question:

> **Can we recover the original vector after applying a transformation?**

The answer depends on whether the **transformation matrix is invertible**.

---

## 1. Idea of Reversibility

A linear transformation is given by:
$[
\mathbf{x'} = T\mathbf{x}
]$

If the inverse matrix $T^{-1}$ exists, then we can recover $\mathbf{x}$ by:
$[
\mathbf{x} = T^{-1}\mathbf{x'}
]$

‚úîÔ∏è **Reversible transformation ‚áî inverse exists**

---

## 2. Example: Reversible Transformation

### Given

Original vector:
$[
\mathbf{u} =
\begin{bmatrix}
3 \\
-2
\end{bmatrix}
]$

Transformation matrix:
$[
T =
\begin{bmatrix}
3 & 2 \\
1 & 2
\end{bmatrix}
]$

---

### Step 1: Apply the Transformation

$[
\mathbf{u'} = T\mathbf{u}
=
\begin{bmatrix}
3 & 2 \\
1 & 2
\end{bmatrix}
\begin{bmatrix}
3 \\
-2
\end{bmatrix}
=
\begin{bmatrix}
5 \\
-1
\end{bmatrix}
]$

So the transformed vector is:
$[
\mathbf{u'} =
\begin{bmatrix}
5 \\
-1
\end{bmatrix}
]$

---

### Step 2: Find the Inverse of $T$

Determinant:
$[
\det(T) = (3)(2) - (2)(1) = 6 - 2 = 4 \neq 0
]$

Since the determinant is **non-zero**, $T$ is invertible.

Inverse of a $2 \times 2$ matrix:
$[
T^{-1} = \frac{1}{\det(T)}
\begin{bmatrix}
d & -b \\
-c & a
\end{bmatrix}
]$

$[
T^{-1} =
\frac{1}{4}
\begin{bmatrix}
2 & -2 \\
-1 & 3
\end{bmatrix}
]$

---

### Step 3: Recover the Original Vector

$[
T^{-1}\mathbf{u'} =
\frac{1}{4}
\begin{bmatrix}
2 & -2 \\
-1 & 3
\end{bmatrix}
\begin{bmatrix}
5 \\
-1
\end{bmatrix}
=
\begin{bmatrix}
3 \\
-2
\end{bmatrix}
]$

‚úîÔ∏è **Original vector recovered**

---

## 3. Example: Non-Reversible Transformation

### Given

Original vector:
$[
\mathbf{u} =
\begin{bmatrix}
3 \\
2
\end{bmatrix}
]$

Transformation matrix:
$[
T =
\begin{bmatrix}
1 & 2 \\
2 & 4
\end{bmatrix}
]$

---

### Step 1: Apply the Transformation

$[
\mathbf{u'} = T\mathbf{u}
=
\begin{bmatrix}
1 & 2  \\
2 & 4
\end{bmatrix}
\begin{bmatrix}
3 \\
2
\end{bmatrix}
=
\begin{bmatrix}
7 \\
14
\end{bmatrix}
]$

---

### Step 2: Check the Determinant

$[
\det(T) = (1)(4) - (2)(2) = 4 - 4 = 0
]$

Since:
$[
\det(T) = 0
]$

‚ùå **Inverse does not exist**

---

### Conclusion

* $T^{-1}$ is undefined
* Original vector **cannot** be recovered
* The transformation is **not reversible**

Such matrices are called **singular matrices**.

---

## 4. Key Rule for Reversibility

A transformation defined by matrix $T$ is **reversible if and only if**:
$[
\det(T) \neq 0
]$

| Determinant      | Inverse Exists? | Reversible? |
| ---------------- | --------------- | ----------- |
| $\det(T) \neq 0$ | Yes             | ‚úÖ Yes       |
| $\det(T) = 0$    | No              | ‚ùå No        |

---

## 5. General Summary

* Forward transformation:
  $[
  \mathbf{x'} = T\mathbf{x}
  ]$

* Reverse transformation (if possible):
  $[
  \mathbf{x} = T^{-1}\mathbf{x'}
  ]$

* Not all linear transformations are reversible

* Reversibility depends **only on the matrix**, not the vector

---

## 6. Important Insight (Geometric Intuition)

* Reversible transformation:

  * No dimension collapse
  * No information loss
* Non-reversible transformation:

  * Squashes space
  * Multiple vectors map to the same output

---

