
# Vector-Valued Functions

## What is a Vector-Valued Function?

A **scalar-valued function** produces a single number as output.

Example:
$$
f(x) = x^2 + 3x + 1
$$

Even multivariable scalar functions return a single value:
$$
f(x,y) \in \mathbb{R}
$$

---

## Vector-Valued Function

A **vector-valued function** returns a vector (multiple outputs):

$$
\mathbf{F}(t) =
\begin{bmatrix}
x(t) \
y(t)
\end{bmatrix}
$$

Example:
$$
\mathbf{F}(t) =
\begin{bmatrix}
t + t^2 \
2 + t
\end{bmatrix}
$$

* Input: one variable (t)
* Output: a vector in (\mathbb{R}^2)

Each component is an independent scalar function.

---

## Physical Interpretation

Let:

* (x(t)) = displacement of a car
* (y(t)) = velocity of the car

Even though the units differ, the vector function is valid since components are independent.

---

## Derivative of a Vector-Valued Function

The derivative is taken **component-wise**:

$$
\mathbf{F}'(t) =
\begin{bmatrix}
x'(t) \
y'(t)
\end{bmatrix}
$$

For the example:
$$
\mathbf{F}(t) =
\begin{bmatrix}
t + t^2 \
2 + t
\end{bmatrix}
$$

$$
\mathbf{F}'(t) =
\begin{bmatrix}
1 + 2t \
1
\end{bmatrix}
$$

Interpretation:

* First component → velocity
* Second component → acceleration

---

## Multivariable Vector-Valued Functions

Vector functions can depend on multiple inputs:

$$
\mathbf{F}(x,y) =
\begin{bmatrix}
x - y \
x^2 + y^2
\end{bmatrix}
$$

* Input: ((x,y))
* Output: vector in (\mathbb{R}^2)

---

## Why Vector-Valued Functions?

* Compact notation
* Group related quantities
* Essential for Jacobians and optimisation

---

# The Jacobian (Scalar Output)

## Scalar Function of Multiple Variables

Consider:
$$
f(x,y) = xy^2 + \sin x
$$

This function has:

* One output
* Two inputs

---

## Partial Derivatives

$$
\frac{\partial f}{\partial x} = y^2 + \cos x
$$

$$
\frac{\partial f}{\partial y} = 2xy
$$

---

## Jacobian Vector

The Jacobian is the row vector:

$$
\mathbf{J}_f =
\begin{bmatrix}
\frac{\partial f}{\partial x} &
\frac{\partial f}{\partial y}
\end{bmatrix}
$$

---

## Jacobian at a Point

At point ((0,1)):

$$
\mathbf{J}_f(0,1) =
\begin{bmatrix}
1 & 0
\end{bmatrix}
$$

Interpretation:

* Slope in (x)-direction is positive
* No change in (y)-direction

---

## Geometric Meaning

* Jacobian represents slope in all coordinate directions
* Generalisation of the derivative to multivariable functions

---

# Jacobian Matrix (Vector-Valued Functions)

## General Case

$$
\mathbf{F}(x,y) =
\begin{bmatrix}
f_1(x,y) \
f_2(x,y)
\end{bmatrix}
$$

---

## Jacobian Matrix Definition

$$
\mathbf{J} =
\begin{bmatrix}
\frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y} \
\frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y}
\end{bmatrix}
$$

* Rows → outputs
* Columns → inputs

---

## General Form

For (m) outputs and (n) inputs:

$$
\mathbf{J}_{ij} = \frac{\partial f_i}{\partial x_j}
$$

The Jacobian is an (m \times n) matrix.

---

## Why the Jacobian Matrix Matters

* Used in multivariate optimisation
* Used in backpropagation
* Gives local linear approximation of functions

---

# The Hessian Matrix

## Definition

The **Hessian** contains second-order partial derivatives.

For a scalar function (f(x_1, x_2, \dots, x_n)):

$$
\mathbf{H} =
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \
\vdots & \ddots & \vdots \
\frac{\partial^2 f}{\partial x_n \partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

General element:
$$
H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
$$

---

## Example

Let:
$$
f(x,y,z) = x^2 + y^2 + z^2
$$

First derivatives:
$$
\frac{\partial f}{\partial x} = 2x, \quad
\frac{\partial f}{\partial y} = 2y, \quad
\frac{\partial f}{\partial z} = 2z
$$

Second derivatives (Hessian):

$$
\mathbf{H} =
\begin{bmatrix}
2 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 2
\end{bmatrix}
$$

---

## Geometric Meaning of the Hessian

* Hessian measures **curvature**
* Large values → sharply curved surface
* Small values → flat region

---

## Jacobian vs Hessian

| Concept             | Measures  | Order  |
| ------------------- | --------- | ------ |
| Jacobian / Gradient | Slope     | First  |
| Hessian             | Curvature | Second |

Perfect — here are the **clean, corrected notes**, with **every formula enclosed in `$$ $$`**, clear wording, and no missing math.

---

# Taylor Series Approximation

Let’s revisit an important concept in calculus that is widely used in mathematics, data science, and machine learning: **Taylor series**.

Taylor series allow us to **approximate complicated functions using polynomials**. These approximations are extremely useful in:

* Optimisation
* Numerical methods
* Machine learning model derivations
* Understanding gradients and curvature

---

## Core Idea of Taylor Series

Using Taylor expansion, a smooth function can be approximated as:

* A **constant term**
* Plus a **linear term** (first-order derivative / gradient)
* Plus a **quadratic term** (second-order derivative / curvature)
* And so on

In other words:

* **First-order Taylor** → linear approximation
* **Second-order Taylor** → quadratic approximation
* Higher orders → more accurate approximations

---

## Taylor Series of a Single-Variable Function

Suppose:

* $$f(x)$$ is **infinitely differentiable**
* We expand the function around the point $$x = a$$

### Taylor Series Expansion About $$x = a$$

$$
f(x) =
f(a) * f'(a)(x - a) * \frac{f''(a)}{2!}(x - a)^2 * \frac{f'''(a)}{3!}(x - a)^3 * \cdots $$

This infinite sum is called the **Taylor series of $$f(x)$$ about $$a$$**.

---

## Compact Sigma Notation

The Taylor series can also be written as:

$$
f(x) = \sum_{n=0}^{\infty}
\frac{f^{(n)}(a)}{n!}(x - a)^n
$$

Where:

* $$f^{(n)}(a)$$ is the $$n$$-th derivative of $$f$$ evaluated at $$a$$
* $$n!$$ is factorial:
  $$
  n! = n \cdot (n-1) \cdot \dots \cdot 1
  $$

---

## Alternative Form Using Increment $$h$$

Sometimes, Taylor series is written by replacing:

* $$a$$ with $$x$$
* $$x - a$$ with $$h$$

Assume the function is continuous and $$n$$ times differentiable on the interval $$[x, x+h]$$.

### Taylor Expansion in Terms of $$h$$

$$
f(x + h) =
f(x)
$$
* $$h f'(x)$$
* $$\frac{h^2}{2!} f''(x)$$
* $$\frac{h^3}{3!} f'''(x)$$
* $$\cdots
  $$

This form is **extremely important** in:

* Numerical analysis
* Error estimation
* Gradient-based optimisation

---

## First-Order Taylor Approximation (Linearisation)

Keeping only the first two terms:

$$
f(x) \approx f(a) + f'(a)(x - a)
$$

This is called **linear approximation**.

Geometric meaning:

* Approximates the function using the **tangent line** at $$x = a$$

---

## Second-Order Taylor Approximation

Keeping terms up to the second derivative:

$$
f(x) \approx
f(a)
$$

* $$f'(a)(x - a)$$
* $$\frac{f''(a)}{2}(x - a)^2
  $$

Geometric meaning:

* Captures **curvature**
* Uses a **parabola** instead of a straight line

---

## Why Taylor Series Matters in Machine Learning

* **Gradient descent** uses first-order Taylor approximation
* **Newton’s method** uses second-order Taylor approximation
* Loss functions are locally approximated using Taylor expansions
* Helps understand why gradients and Hessians matter

---

## Connection to Multivariable Calculus

For a multivariable function $$f(\mathbf{x})$$, the second-order Taylor expansion around $$\mathbf{x}_0$$ is:

$$
f(\mathbf{x}) \approx
f(\mathbf{x}_0)
$$
$$
\nabla f(\mathbf{x}_0)^T (\mathbf{x} - \mathbf{x}_0)
\frac{1}{2} (\mathbf{x} - \mathbf{x}_0)^T
$$
$$
 \mathbf{H}
  (\mathbf{x} - \mathbf{x}_0)
  $$

Where:

* $$\nabla f$$ is the **gradient**
* $$\mathbf{H}$$ is the **Hessian matrix**

This formula is the **mathematical backbone of optimisation algorithms**.

---

## Summary

* Taylor series approximates functions using derivatives
* First-order → slope (gradient)
* Second-order → curvature (Hessian)
* Higher order → higher accuracy
* Fundamental to optimisation and ML theory

---

