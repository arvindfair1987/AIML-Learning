
# Linear Transformations 

## Why Linear Transformations Matter

Linear algebra is essentially the **study of linear transformations**.
A transformation moves elements in space, but in linear algebra we care about **special transformations** that preserve the algebraic structure of the space.

Linear transformations:

* Respect **addition**
* Respect **scalar multiplication**

Matrices exist mainly to **represent linear transformations**.
In fact, **matrix multiplication was originally defined to describe linear transformations**.

Because of this, if we want to build a complete algebraic system using matrices (similar to real numbers), we must understand linear transformations first.

---

## Connection to Data Science & Machine Learning

Many real-world problems require transforming high-dimensional data while preserving important structure:

* **Machine Learning**
* **Data Compression**
* **Signal Processing**
* **Quantum Computing**

Two core concepts enable this:

### Eigenvalues and Eigenvectors

* Fundamental to understanding how transformations act on space
* Used in:

  * Principal Component Analysis (PCA)
  * Singular Value Decomposition (SVD)
  * Dimensionality reduction
  * Feature extraction

You **cannot fully understand PCA or SVD** without understanding eigenvalues and eigenvectors.

---

## Topics Covered in This Session

1. Linear transformations
2. Changing bases and transformations in different bases
3. Eigenvalues and eigenvectors
4. Finding eigenvalues and eigenvectors
5. Eigendecomposition of a matrix

---

# Recap: Vectors for Data Science

Before studying transformations, we revise vectors â€” the **language of data**.

### Why Vectors Are Important

All data is represented numerically as vectors:

* Customer behavior
* Image pixels
* Word embeddings (NLP)

Vectors allow us to:

* Measure distances
* Compute similarity
* Transform data mathematically

Understanding vectors builds the foundation for:

* Machine learning
* Neural networks
* Deep learning

---

## Vector Topics Reviewed

* Geometric representation of vectors
* Vector operations
* Linear combinations
* Basis vectors
* Vector spaces

---

## What Is a Vector?

A **vector** has:

* **Magnitude** (length)
* **Direction**

This distinguishes vectors from **scalars**, which only have magnitude.

---

## One-Dimensional Vector Example

Consider temperature readings over time:

| Point | Temperature |
| ----- | ----------- |
| A     | 30          |
| B     | 35          |
| C     | -40         |

Although these look like scalars, they can be represented as **vectors on a number line**.

* The number line is the **temperature axis**
* There is only **one dimension**
* Each vector:

  * Starts at 0 (tail)
  * Ends at the temperature value (head)

Examples:

* **A = 30**

  * Length = 30
  * Direction = positive
* **C = -40**

  * Length = 40
  * Direction = negative

In one dimension, direction is limited to **positive or negative along the same axis**.

---

## Two-Dimensional Vector Example

When data has **two attributes**, we move to two dimensions.

Example:

* Longitude
* Latitude

Each data point becomes a vector in a 2D plane.

---

## Three-Dimensional Vector Example

Consider cities with latitude and longitude data.

Example:

* City: Rourkela
* Longitude = 77
* Latitude = 29

To represent this:

1. Use two axes:

   * X-axis â†’ Longitude
   * Y-axis â†’ Latitude
2. Start from the origin
3. Move:

   * 77 units along longitude
   * 29 units along latitude
4. The final point is the **head of the vector**

This vector now carries:

* **Magnitude**
* **Direction in 2D space**

Perfect â€” Iâ€™ll **clean, organize, and tighten this into clear lecture notes**, same style as before. Iâ€™ll fix language, remove repetition, and keep intuition front and center.

---

# Cartesian and Polar Coordinates

Using the **origin as the tail** and the endpoint as the **head**, we draw an arrow.
This arrow itself **is the vector**.

Every vector has:

* **Length (magnitude)**
* **Direction**

Two vectors may have the same length but **different directions**, making them completely different vectors.

---

## Magnitude and Direction (Geographical Intuition)

Think in geographical terms:

* **0Â° longitude** â†’ Prime Meridian
* **0Â° latitude** â†’ Equator

If a city has coordinates (77, 29):

* Move **77 units east** (longitude)
* Move **29 units north** (latitude)

The arrow from the origin to this point:

* Tells us **how far** the city is
* Tells us **which direction** it lies in

That is exactly what a vector represents.

---

## Three-Dimensional Vector Example

Now extend this idea to **three dimensions**.

Suppose we analyze hostels using:

* X-axis â†’ number of **fans**
* Y-axis â†’ number of **coolers**
* Z-axis â†’ number of **ACs**

Example:

* Hostel *Shakti* has:

  * 2 fans
  * 3 coolers
  * 5 ACs

This is represented by the vector:
$$[
(2, 3, 5)
]$$

Steps:

1. Move 2 units along X
2. Move 3 units along Y
3. Move 5 units along Z

The resulting arrow uniquely represents *Hostel Shakti* in 3D space.

Plotting multiple hostels as vectors allows:

* Visual comparison
* Similarity analysis
* Distance-based reasoning

This idea is heavily used in **data analysis and clustering**.

---

## Cartesian Representation of Vectors

Vectors are usually described using **components**.

In 2D Cartesian coordinates:

* A vector **U** has components:

  * (U_1): distance from Y-axis (X-coordinate)
  * (U_2): distance from X-axis (Y-coordinate)

From the origin:

* Move (U_1) units along X
* Move (U_2) units along Y

This fully defines:

* The **length**
* The **direction**
* The **position**

---

## Polar Coordinate Representation

The **same vector** can be represented differently using **polar coordinates**.

Instead of ((U_1, U_2)), we use:

* **r** â†’ magnitude (length of vector)
* **Î¸** â†’ angle from the X-axis

Interpretation:

1. Start with a vector of length **r** along X-axis
2. Rotate it by angle **Î¸**

This lands you at the same vector.

---

## Magnitude (Norm) of a Vector

The **magnitude** of a vector is the **length of the arrow**.

For a 2D vector:
$$[
\mathbf{U} = (U_1, U_2)
]$$

Magnitude (also called **norm**) is:
$$[
|\mathbf{U}| = \sqrt{U_1^2 + U_2^2}
]$$

This is simply the **Euclidean distance** from the origin.

---

### n-Dimensional Case

For an n-dimensional vector:
$$[
|\mathbf{U}| = \sqrt{U_1^2 + U_2^2 + \cdots + U_n^2}
]$$

Same idea â€” just extended to more dimensions.

---

## Unit Vectors

A **unit vector** is a vector whose magnitude is **1**.

Purpose:

* Represents **direction only**
* Removes magnitude information

### How to Compute a Unit Vector

Given a vector $(\mathbf{U})$:
$$[
\hat{\mathbf{U}} = \frac{\mathbf{U}}{|\mathbf{U}|}
]$$

---

### Example

Let:
$$[
\mathbf{U} = (4, 3)
]$$

Magnitude:
$$[
|\mathbf{U}| = \sqrt{4^2 + 3^2} = 5
]$$

Unit vector:
$$[
\hat{\mathbf{U}} = \left(\frac{4}{5}, \frac{3}{5}\right)
]$$

This vector:

* Points in the **same direction**
* Has **length 1**

---

## Standard Unit Vectors

### In 2D

* **iÌ‚** = (1, 0) â†’ X-axis
* **Äµ** = (0, 1) â†’ Y-axis

### In 3D

* **iÌ‚** = (1, 0, 0)
* **Äµ** = (0, 1, 0)
* **kÌ‚** = (0, 0, 1)

These are the **building blocks** of vector spaces.

---

## Zero Vector

The **zero vector**:

* Has magnitude **0**
* Is **not the same as scalar 0**

Notation:

* **$(\mathbf{0})$** or **$0Ì…$**

Examples:

* 1D: (0)
* 2D: (0, 0)
* 3D: (0, 0, 0)

Every vector space has **exactly one zero vector**.

Great, continuing in the same flow â€” here are **clean, structured notes** for this segment. Iâ€™ll also gently correct terminology where needed (because the heading says *Linear Independence*, but the content here is really **vector addition + basis foundations**).

---

# Vector Addition and Resultant Vectors

## Geometric Interpretation of Vector Addition

To add two vectors **U** and **V** geometrically:

1. Place the **tail of V at the head of U**
2. This forms a triangle
3. The **third side of the triangle** (from the tail of U to the head of V) is the **resultant vector**

This resultant vector represents:
$$[
\mathbf{U} + \mathbf{V}
]$$

---

## Parallelogram Method

Another equivalent way to add vectors:

* Place both vectors **U and V tail-to-tail**
* Complete a **parallelogram**
* The **diagonal** of the parallelogram is the resultant vector

Both triangle and parallelogram methods give the **same result**.

---

## Example: Adding Two 2D Vectors

Let:

* **S = (20, 50)** â†’ blue vector
* **M = (50, 20)** â†’ green vector

### Component-wise Addition

X-components:
$[
20 + 50 = 70
]$

Y-components:
$[
50 + 20 = 70
]$

So the resultant vector is:
$$[
\mathbf{S} + \mathbf{M} = (70, 70)
]$$

This is shown as the **red arrow** in the diagram.

---

## Algebraic Rule for Vector Addition

For two **n-dimensional vectors**:

$$[
\mathbf{U} = (U_1, U_2, \dots, U_n)
]$$
$$[
\mathbf{V} = (V_1, V_2, \dots, V_n)
]$$

Their sum is:
$$[
\mathbf{U} + \mathbf{V} = (U_1 + V_1,; U_2 + V_2,; \dots,; U_n + V_n)
]$$

ðŸ‘‰ Vector addition is always done **component by component**.

---

# Properties of Vector Addition

Vector addition follows several important algebraic properties.

---

## 1. Commutativity

$$[
\mathbf{A} + \mathbf{B} = \mathbf{B} + \mathbf{A}
]$$

Geometrically:

* Start with **A**, then add **B**
* Or start with **B**, then add **A**
* You reach the **same final point**

---

## 2. Associativity

$$[
(\mathbf{A} + \mathbf{B}) + \mathbf{C} = \mathbf{A} + (\mathbf{B} + \mathbf{C})
]$$

No matter how vectors are grouped, the result is the same.

---

## 3. Additive Identity (Zero Vector)

There exists a vector **0Ì…** such that:
$$[
\mathbf{A} + \mathbf{0} = \mathbf{A}
]$$

The zero vector does **nothing** when added.

---

## 4. Additive Inverse

For every vector **A**, there exists **âˆ’A** such that:
$$[
\mathbf{A} + (-\mathbf{A}) = \mathbf{0}
]$$

Geometrically:

* Same magnitude
* Opposite direction

---

# Basis and Basis Vectors (Concept Preview)

The idea of component-wise addition naturally leads to the concept of a **basis**.

A **basis** is a set of vectors that:

1. Can be combined to represent **every vector in the space**
2. Are **linearly independent**

Examples:

* In 2D: $({\hat{i}, \hat{j}})$
* In 3D: $({\hat{i}, \hat{j}, \hat{k}})$

Any vector can be written as a combination of basis vectors.

---

# Orthogonal and Orthonormal Basis (Concept Preview)

* **Orthogonal basis**: basis vectors are perpendicular to each other
* **Orthonormal basis**:

  * Orthogonal
  * Each vector has **unit length**

Standard Cartesian basis is **orthonormal**.

---

# Important Note on Linear Independence

Although not fully covered yet, this section sets the stage for:

### Linear Dependence

A vector can be written as a combination of others.

### Linear Independence

No vector in the set can be written as a combination of the others.

This will become critical when we define:

* Bases
* Dimension
* Linear transformations

---

## Whatâ€™s Next

Next natural steps:

* Formal definition of **linear independence vs dependence**
* **Span** of vectors
* **Basis and dimension**
* Transition to **linear transformations**

Perfect â€” letâ€™s move forward **cleanly and conceptually**, no gaps.
Here are the **next natural steps**, written as **exam-ready + intuition-first notes**.

---

# Linear Independence and Dependence

## Why This Matters

When we talk about **basis**, **dimension**, and **linear transformations**, everything depends on one idea:

> Are some vectors **redundant**, or does each one add **new information**?

That question is answered by **linear independence**.

---

## Linear Combination

A **linear combination** of vectors means:

$$[
a_1\mathbf{v}_1 + a_2\mathbf{v}_2 + \dots + a_n\mathbf{v}_n
]$$

where:

* (a_1, a_2, \dots, a_n) are **scalars**
* (\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_n) are vectors

This is how vectors are **built from other vectors**.

---

## Linear Dependence

A set of vectors is **linearly dependent** if:

> At least one vector can be written as a linear combination of the others.

### Equivalent Statement

There exist scalars (not all zero) such that:

$$[
a_1\mathbf{v}_1 + a_2\mathbf{v}_2 + \dots + a_n\mathbf{v}_n = \mathbf{0}
]$$

---

### Geometric Intuition

* In **2D**:

  * Two vectors on the **same line** â†’ dependent
* In **3D**:

  * Three vectors in the **same plane** â†’ dependent

ðŸ‘‰ One vector does **not add a new direction**.

---

### Example (Dependent)

$$[
\mathbf{v}_1 = (2, 4), \quad \mathbf{v}_2 = (1, 2)
]$$

Here:
$$[
\mathbf{v}_1 = 2\mathbf{v}_2
]$$

So these vectors are **linearly dependent**.

---

## Linear Independence

A set of vectors is **linearly independent** if:

> The only way to get the zero vector is by choosing **all scalars = 0**.

$$[
a_1\mathbf{v}_1 + a_2\mathbf{v}_2 + \dots + a_n\mathbf{v}_n = \mathbf{0}
\Rightarrow a_1 = a_2 = \dots = a_n = 0
]$$

---

### Geometric Intuition

* Each vector points in a **new direction**
* None can be built from the others
* No redundancy

---

### Example (Independent)

$$[
\mathbf{v}_1 = (1, 0), \quad \mathbf{v}_2 = (0, 1)
]$$

No scalar multiple of one gives the other â†’ **independent**.

---

## Span of Vectors

The **span** of a set of vectors is:

> The set of all possible linear combinations of those vectors.

---

### Examples

* Span of ((1, 0)) â†’ X-axis
* Span of ((1, 0), (0, 1)) â†’ entire 2D plane
* Span of dependent vectors â†’ **smaller space**

---

## Basis

A **basis** is a set of vectors that:

1. **Spans** the space
2. Is **linearly independent**

---

### Key Insight

> A basis is the **smallest set of vectors** needed to describe the space **without redundancy**.

---

### Examples

* 1D space â†’ 1 basis vector
* 2D space â†’ 2 basis vectors
* 3D space â†’ 3 basis vectors

Standard bases:

* 2D: ({\hat{i}, \hat{j}})
* 3D: ({\hat{i}, \hat{j}, \hat{k}})

---

## Dimension

The **dimension** of a vector space is:

> The number of vectors in any basis of the space.

Examples:

* Line â†’ dimension 1
* Plane â†’ dimension 2
* Space â†’ dimension 3

---

## Why This Is Crucial for Transformations

Linear transformations:

* Act on **basis vectors**
* Automatically determine what happens to **all other vectors**

If you know:

* A basis
* How a transformation moves that basis

You know the **entire transformation**.

---

## Where This Leads Next

Now we are perfectly set up for:

### ðŸ”œ Linear Transformations

* Definition
* Properties
* Matrix representation

### ðŸ”œ Change of Basis

* Same vector, different coordinates
* Why PCA works

### ðŸ”œ Eigenvectors & Eigenvalues

* Directions that **do not change**
* Only scale

---

Perfect â€” Iâ€™ll rewrite the **same notes**, but now **all mathematical expressions are enclosed in `$ ... $` or `$$ ... $$`**, so you can paste directly into a `.md` file without fixing anything.

---

# MATRICES â€” CLEAN & COMPLETE NOTES

*(Data Science & Machine Learning Perspective)*

---

## 1. Why Matrices Matter in Data Science & Machine Learning

Matrices are everywhere in data science:

* **Datasets** â†’ rows = samples, columns = features
* **Images** â†’ pixels arranged in matrices
* **Text embeddings** â†’ vectors stacked into matrices
* **Neural networks** â†’ weights are matrices

> **Vectors give direction.
> Matrices give structure and transformation.**

Matrices help us:

* Represent complex systems compactly
* Perform fast computations
* Model relationships between variables
* Scale algorithms to large datasets

---

## 2. What Is a Matrix?

### Definition

A **matrix** is a **rectangular array of numbers** arranged in **rows and columns**.

Example:

$$
A =
\begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix}
$$

* Each number is an **element**
* Horizontal entries â†’ **rows**
* Vertical entries â†’ **columns**

---

### Dimensions (Order of a Matrix)

If a matrix has:

* $m$ rows
* $n$ columns

Then its dimension is:

$$
m \times n
$$

Example above is a **$3 \times 3$ matrix**.

---

## 3. Matrix Representation of Data

Tabular data can be represented as matrices.

Example: Height & Weight data

| Person | Height | Weight |
| ------ | ------ | ------ |
| 1      | 155    | 60     |
| 2      | 171    | 72     |
| 3      | 165    | 68     |

Matrix form:

$$
\begin{bmatrix}
155 & 60 \\
171 & 72 \\
165 & 68
\end{bmatrix}
$$

* Rows â†’ individuals
* Columns â†’ features

---

## 4. Matrix Addition

### Rule

Two matrices can be added **only if they have the same dimensions**.

$$
A + B = \text{element-wise addition}
$$

Example:

$
\begin{bmatrix}
2 & 3 \\
4 & 5
\end{bmatrix}
$ +
$
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix}
$
=
$
\begin{bmatrix}
3 & 4 \\
6 & 7
\end{bmatrix}
$

### Applications

* Combining production data
* Aggregating datasets
* Summing statistics

ðŸš« Matrices of different sizes **cannot be added**.

---

## 5. Matrix Subtraction

Same condition as addition:

$$
A - B = \text{element-wise subtraction}
$$

Used for:

* Finding pending amounts
* Error calculation
* Difference between expected and actual values

---

## 6. Scalar Multiplication

A **scalar** is a single number.

Multiply every element of a matrix by a scalar $k$:

$
kA =
k \times
$
$
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
$
=
$
\begin{bmatrix}
ka & kb \\
kc & kd
\end{bmatrix}
$

### Data Science intuition

* Feature scaling
* Weight adjustment
* Normalization

---

## 7. Types of Matrices

### 7.1 Square Matrix

A matrix where:

$$
\text{rows} = \text{columns}
$$

Example: $3 \times 3$, $4 \times 4$

---

### 7.2 Diagonal Matrix

A **square matrix** where all non-diagonal elements are zero.

$$
\begin{bmatrix}
5 & 0 & 0 \\
0 & -2 & 0 \\
0 & 0 & 7
\end{bmatrix}
$$

---

### 7.3 Scalar Matrix

A **diagonal matrix** where all diagonal elements are equal.

$$
\begin{bmatrix}
2 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 2
\end{bmatrix}
$$

---

### 7.4 Identity Matrix

A scalar matrix with diagonal elements equal to $1$.

Denoted by $I$.

$$
I =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
$$

Key property:

$$
AI = IA = A
$$

---

### 7.5 Zero (Null) Matrix

A matrix where **all elements are zero**.

$$
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix}
$$

---

## 8. Matrix Transpose

### Definition

The transpose of a matrix is obtained by **swapping rows and columns**.

Denoted by $A^T$.

Example:

$$ \begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}^T$$
=
$$
\begin{bmatrix}
1 & 4 \\
2 & 5 \\
3 & 6
\end{bmatrix}
$$

### Properties

$$
(A^T)^T = A
$$

$$
(A + B)^T = A^T + B^T
$$

$$
(AB)^T = B^T A^T
$$

---

## 9. Matrix Multiplication

### Rule

If:

* $A$ is $m \times n$
* $B$ is $n \times p$

Then:

$$
AB \text{ is } m \times p
$$

ðŸš« If inner dimensions do not match, multiplication is not possible.

---

### Element Computation

$$
C_{ij} = \sum_k A_{ik} B_{kj}
$$

### Important Property

$$
AB \neq BA
$$

(Matrix multiplication is **not commutative**)

---

## 10. Matrixâ€“Vector Multiplication

A vector is a matrix with one column.

$$
A\vec{x}
$$

Interpretation:

* Linear combination of columns of $A$
* Core idea behind linear models and neural networks

---

## 11. Determinants

### Definition

A **determinant** is a single real number associated with a **square matrix**.

$$
\det(A)
$$

---

### Geometric Interpretation

* $2 \times 2$ matrix â†’ area of parallelogram
* $3 \times 3$ matrix â†’ volume of parallelepiped

$$
|\det(A)| = \text{area or volume}
$$

---

### Special Cases

$$
\det(A) = 0 \Rightarrow \text{singular matrix}
$$

$$
\det(A) \neq 0 \Rightarrow \text{invertible matrix}
$$

---

## 12. Inverse of a Matrix

### Definition

The inverse of $A$ is $A^{-1}$ such that:

$$
AA^{-1} = A^{-1}A = I
$$

### Condition

$$
\det(A) \neq 0
$$

Used in:

* Solving linear equations
* Linear regression
* Optimization problems

---

## 13. System of Linear Equations

General form:

$$
AX = B
$$

Where:

* $A$ â†’ coefficient matrix
* $X$ â†’ unknown vector
* $B$ â†’ output vector

### Solutions

| Condition        | Result                            |
| ---------------- | --------------------------------- |
| $\det(A) \neq 0$ | Unique solution                   |
| $\det(A) = 0$    | No solution or infinite solutions |

---

## 14. Why This Matters in ML

Matrices are the backbone of:

* Linear regression
* PCA and eigen decomposition
* Neural networks
* Gradient-based optimization

> **Understanding matrices = understanding how machine learning works under the hood.**



Got it â€” this looks like **raw lecture transcript notes**, and what you want is a **clean, structured, exam-ready version**. Iâ€™ll **organize, remove repetition, fix flow, and clarify ideas**, without adding unnecessary math.

---

