## Welcome

Welcome to the module **Advanced Mathematics for Data Science**.

This module builds upon foundational conceptsâ€”including arithmetic, basic probability, combinatorics, linear algebra, and calculusâ€”and advances them toward more sophisticated mathematical techniques that are essential for modern data science.

---

## Module Overview

Throughout this module, you will revisit and deepen key principles in probability, moving from discrete to continuous probability distributions. You will also strengthen your understanding of vectors and matrices, along with the core structures of vector spaces such as bases and spans.

In addition, the module explores linear transformations and the role of eigenvalues and eigenvectors in representing, transforming, and interpreting highâ€‘dimensional data.

The module further delves into multivariable calculus, focusing on partial derivatives, gradients, and optimisation methods. These concepts form the mathematical backbone of many data science algorithms and analytical techniques.

---

## Learning Objectives

By the end of this module, you will be able to:

* Analyse and interpret probability density functions for continuous random variables.
* Comprehend and apply advanced probability distributions to analyse diverse data sets.
* Gain proficiency in vector and matrix operations for efficiently solving linear systems.
* Develop a solid understanding of multivariable calculus concepts as a foundation for optimisation problems in data science.

---

## Core Mathematical Pillars

This module is organised around three core mathematical pillars:

1. **Uncertainty Modelling with Probability**
   Probability deals with randomness and provides the foundation for statistical inference. It enables us to model uncertainty, reason about data variability, and make informed predictions.

2. **Data Transformation with Linear Algebra**
   Linear algebra focuses on vectors, matrices, transformation matrices, matrix decompositions, eigenvalues and eigenvectors, and the geometry of data in higherâ€‘dimensional spaces. These tools are central to understanding how data is represented and transformed in machine learning models.

3. **Calculus for Algorithm Optimisation**
   Calculus is concerned with functions, derivatives, and multivariable functions. Topics such as partial differentiation, gradients, and backpropagation are essential for training and optimising machine learning algorithms.

---

## Why This Matters

These mathematical ideas are what give data science and machine learning algorithms their power. They help us understand **why** models behave the way they do and **how** to design better, more efficient algorithms.

Whether you are working in deep learning, Bayesian modelling, timeâ€‘series analysis, or other domains of data science and machine learning, a strong grasp of these mathematical foundations serves as your scaffolding for building robust and interpretable solutions.

---

This module blends theoretical understanding with practical application, equipping you with advanced quantitative skills to navigate and analyse complex data structures and tackle realâ€‘world data science challenges effectively.


Absolutely! Iâ€™ll **expand your notes** to include all the missing topics you mentioned, with clear explanations, examples, and connections. I will integrate them seamlessly into the previous structure.

---

# **Complete Probability Notes (Expanded)**

---

## **1ï¸âƒ£ Introduction to Probability**

Probability helps **quantify and manage uncertainty** â€” i.e., how likely an event is to happen.

$$[
P(E) = \frac{\text{Number of favorable outcomes}}{\text{Total number of possible outcomes}}
]$$

**Rules of Probability:**

1. $$(0 \le P(E) \le 1)$$
2. Sum of probabilities of all outcomes in a sample space equals 1:
   
$$[
   \sum P(E_i) = 1
   ]$$

---

## **2ï¸âƒ£ Key Concepts**

| Term           | Meaning                                                |
| -------------- | ------------------------------------------------------ |
| Experiment     | A process producing outcomes. *E.g., rolling a die.*   |
| Trial          | Single execution of an experiment.                     |
| Outcome        | Possible result of a trial. *E.g., rolling a 4.*       |
| Sample Space S | Set of all possible outcomes. *S = {1,2,3,4,5,6}*      |
| Event E        | Any subset of sample space. *E.g., E_even = {2,4,6}*   |
| Null Event âˆ…   | Event that never occurs. *E.g., rolling a 0 on a die.* |

---

## **3ï¸âƒ£ Types of Events**

| Type               | Definition                               | Example                           |
| ------------------ | ---------------------------------------- | --------------------------------- |
| Elementary         | Single outcome                           | {1}                               |
| Compound           | Multiple outcomes                        | {2,4,6}                           |
| Null               | Empty set                                | âˆ…                                 |
| Mutually Exclusive | Cannot occur together                    | {1}, {2}                          |
| Exhaustive         | Cover entire sample space                | {1,2,3}, {4,5,6}                  |
| Independent        | Occurrence of one does not affect others | Tossing coins                     |
| Dependent          | Occurrence of one affects others         | Drawing cards without replacement |

---

## **4ï¸âƒ£ Frequency & Probability**

* **Frequency (n(E))**: Number of times an event occurs in an experiment.
* **Number of Trials (N)**: Total trials in an experiment.

**Formula for mutually exclusive & exhaustive events:**

$$[
\sum_{k=1}^{K} n(E_k) = N
]$$

---

### **4.1 Empirical vs Theoretical Probability**

**Empirical Probability** (based on experiment):

$$[
P(E) = \frac{n(E)}{N}
]$$

**Theoretical Probability** (based on assumptions, no experiments):

$$[
P(E) = \frac{\text{Number of favorable outcomes}}{\text{Total outcomes in S}}
]$$

**Connection to Law of Large Numbers:**

> As the number of trials (N \to \infty), **empirical probability converges to theoretical probability**.
> *Example:* Rolling a die 10 times â†’ empirical probability fluctuates; 1000 times â†’ approaches 1/6.

---

## **5ï¸âƒ£ Approaches to Probability**

1. **Equal Likelihood Approach**: Assumes all outcomes are equally likely.
   *Example:* Rolling a fair die â†’ each face has P = 1/6.

2. **Relative Frequency Approach**: Based on observed outcomes in experiments.
   *Example:* Roll die 60 times â†’ 1 appears 12 times â†’ P(1) = 12/60 = 0.2.

3. **Judgmental / Subjective Approach**: Based on intuition, past experience, or expert opinion.
   *Example:* Probability of rain tomorrow = 0.7 (based on weather forecast).

---

## **6ï¸âƒ£ Complement Rule**

* **Complement of an event A**: Event that A does **not** occur, denoted (A^c).

$$
[
P(A^c) = 1 - P(A)
]$$

* **Connection to Null Event:**

  * If $$(A = S) (entire sample space) â†’ (A^c = âˆ…)$$
  * If $$(A = âˆ…) â†’ (A^c = S)$$

*Example:* Roll a die. Event A = {2,4,6} (even).

$$[
P(A) = 3/6 = 0.5
]$$


$$[
P(A^c) = 1 - 0.5 = 0.5 \text{ (odd numbers: 1,3,5)}
]$$

---

## **7ï¸âƒ£ Rules of Probability**

### **7.1 Addition Rule**


$$[
P(A \cup B) = P(A) + P(B) - P(A \cap B)
]$$

* If A and B are **mutually exclusive**:
  
$$[
  P(A \cup B) = P(A) + P(B)
  ]$$

*Example:* Probability of drawing **king or red card**:

$$[
P(A \cup B) = \frac{4}{52} + \frac{26}{52} - \frac{2}{52} = 28/52
]$$

### **7.2 Multiplication Rule**

$$[
P(A \cap B) = P(A) \cdot P(B|A)
]$$

* If **independent events**:
  
$$[
  P(A \cap B) = P(A) \cdot P(B)
  ]$$

*Examples:*

1. **Consecutive heads in coin tosses (independent):**

$$[
   P(H \cap H) = 1/2 \cdot 1/2 = 1/4
   ]$$
2. **Two kings consecutively (dependent):**
   
$$[
   P(K_1 \cap K_2) = 4/52 \cdot 3/51 = 1/221
   ]$$

---

## **8ï¸âƒ£ Joint, Marginal, Conditional Probability**

**Scenario:** Class of students: Gender (M/F) & Exam Outcome (Pass/Fail)

| Gender\Outcome | Pass | Fail | Total |
| -------------- | ---- | ---- | ----- |
| Male           | 0.3  | 0.2  | 0.5   |
| Female         | 0.25 | 0.25 | 0.5   |
| Total          | 0.55 | 0.45 | 1     |

* **Joint Probability:** Both characteristics together, e.g., P(Male âˆ© Pass) = 0.3
* **Marginal Probability:** One characteristic ignoring others, e.g., P(Male) = 0.5
* **Conditional Probability:** One event given another:
  
$$[
  P(Fail | Female) = 0.25 / 0.5 = 0.5
  ]$$

---

## **9ï¸âƒ£ Bayesâ€™ Theorem**

**Formula:**

$$[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
]$$

**Example:**

* Disease test scenario:

  * P(Disease) = 0.01, P(No Disease) = 0.99
  * Test positive if disease = 0.95, Test positive if no disease = 0.05
  * Probability a person **has disease given positive test**:

$$[
P(D|+) = \frac{P(+|D) \cdot P(D)}{P(+|D)P(D) + P(+|ND)P(ND)}
= \frac{0.95 * 0.01}{0.95*0.01 + 0.05*0.99} \approx 0.161
]$$

> Shows even a positive test has **only 16% chance of true disease** because disease is rare.

---

## **10ï¸âƒ£ Dice Example: Empirical vs Theoretical**

| Outcome | Frequency (n(E)) | Empirical P(E) | Theoretical P(E) |
| ------- | ---------------- | -------------- | ---------------- |
| 1       | 10               | 0.167          | 1/6 = 0.167      |
| 2       | 8                | 0.133          | 0.167            |
| 3       | 12               | 0.2            | 0.167            |
| 4       | 9                | 0.15           | 0.167            |
| 5       | 11               | 0.183          | 0.167            |
| 6       | 10               | 0.167          | 0.167            |

> As number of trials increases, **empirical probability â†’ theoretical probability** (Law of Large Numbers).

---

### âœ… **Key Takeaways**

1. Probability = likelihood of an event (0 to 1)
2. Types of events: independent, dependent, mutually exclusive, exhaustive
3. Probability types: marginal, joint, conditional
4. Rules: addition, multiplication, complement
5. Empirical vs theoretical probability; Law of Large Numbers
6. Conditional probability & Bayesâ€™ theorem for dependent events
7. Three probability approaches: equal likelihood, relative frequency, judgmental


# Conditional Probability and Its Intuition

---

## 1ï¸âƒ£ Recap: What Is Probability?

Probability is defined over a **set of events** and measures how likely an event is to occur.

**General definition:**

$$[
P(E) = \frac{\text{Number of favourable outcomes}}{\text{Total number of outcomes}}
]$$

### Example: Coin Toss

* **Experiment:** Toss a fair coin
* **Event:** Getting a Head (H)

If we toss the coin 100 times and observe that about 50 outcomes are Heads, then:

$$[
P(H) = \frac{50}{100} = 0.5
]$$

This probability is computed **without any additional information**. Such a probability is often called the **unconditional probability** or **prior probability**.

---

## 2ï¸âƒ£ Motivation for Conditional Probability

Now let us see how probabilities change **when new information is available**.

### Playing Example (Intuition Builder)

* Suppose there are **30 days in a month**.
* You like playing outdoors and usually play on **20 days**.

Probability of playing (without any extra information):

$$[
P(\text{Play}) = \frac{20}{30} = 0.67 ; (67%)
]$$

This probability is calculated assuming **all days are equally likely**.

---

## 3ï¸âƒ£ Effect of Additional Information (Conditioning)

Now suppose we are told that **it is going to rain today**.

* When it rains, you rarely play.
* Let us assume you play only **10% of the time when it rains**.

So now the probability of playing **given that it rains** becomes:

$$[
P(\text{Play | Rain}) = 0.10
]$$

### Key Observation

Knowing that **it rains** changes the probability of playing **drastically**.

* Before knowing about rain â†’ probability = 67%
* After knowing about rain â†’ probability = 10%

This is the core idea of **conditional probability**:

> The probability of an event changes when we know that another event has already occurred.

---

## 4ï¸âƒ£ Definition of Conditional Probability

The **conditional probability** of event A given event B is defined as:

$$[
P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0
]$$

### Interpretation:

* We **restrict the sample space** to event B.
* Then we calculate the probability of A **within this restricted space**.

---

## 5ï¸âƒ£ Spam vs Ham Email Example

### Unconditional (Prior) Probability

Suppose:

* 1 out of 5 emails is spam
* 4 out of 5 emails are not spam (ham)

Then:

$$[
P(\text{Spam}) = 0.20
]$$

$$[
P(\text{Ham}) = 0.80
]$$

This is our belief **before seeing any content** of the email.

---

## 6ï¸âƒ£ Conditional Probability in Email Classification

Now suppose we observe that an email **contains the word â€œViagraâ€**.

Given this information, our belief changes.

* Emails containing words like *Viagra* are more likely to be spam.
* Assume that the probability of spam **given the word Viagra appears** is 70%.

$$[
P(\text{Spam | Contains Viagra}) = 0.70
]$$

### Intuition

* The word *Viagra* acts as **additional evidence**.
* This evidence changes the probability of the email being spam.

This idea forms the foundation of:

* Spam filters
* Medical diagnosis
* Machine learning classifiers
* Risk prediction models

---

## 7ï¸âƒ£ Prior vs Conditional Probability

| Concept                 | Meaning                               | Example        |                |
| ----------------------- | ------------------------------------- | -------------- | -------------- |
| Prior Probability       | Probability without extra information | P(Spam) = 0.20 |                |
| Conditional Probability | Probability given new information     | P(Spam         | Viagra) = 0.70 |

---

## 8ï¸âƒ£ Key Intuition Summary

1. Probability depends on **available information**.
2. Conditional probability updates beliefs when new information is known.
3. Conditioning **reduces the sample space**.
4. Conditional probability is central to **Bayesâ€™ Theorem** and real-world decision-making.

---

## 9ï¸âƒ£ Real-World Insight

> *The more relevant information we have, the better our probability estimates become.*

This is why conditional probability is fundamental in:

* Data Science
* Artificial Intelligence
* Statistics
* Economics
* Medical testing

---

### âœ… Takeaway

Conditional probability answers questions of the form:

> **â€œWhat is the probability of A, given that B has already occurred?â€**

Understanding this intuition is essential before learning **Bayesâ€™ Theorem**.

# Introduction to Bayesian Probability

---

## 1ï¸âƒ£ Why Bayesian Probability?

So far, we have discussed **probability** and **conditional probability**, where probabilities change when new information becomes available.

This naturally leads us to a **Bayesian way of thinking about probability**, where probability is viewed as a **degree of belief** that gets updated as we observe new evidence.

Bayesian probability answers the question:

> *How should we update our beliefs when new information is observed?*

To formalize this idea, we introduce two important concepts:

* **Prior Probability**
* **Posterior Probability**

---

## 2ï¸âƒ£ Prior Probability

The **prior probability** represents our belief about an event **before** observing any new information.

### Definition

> **Prior Probability:** The probability of an event without considering any additional evidence.

---

### Example 1: Playing Outdoors

* Total days in a month = 30
* Days you usually play = 20

$$[
P(\text{Play}) = \frac{20}{30} = 0.67 ; (67%)
]$$

This 67% is the **prior probability** of playing, because it is calculated **without knowing whether it rains or not**.

---

### Example 2: Spam vs Ham Emails

Suppose:

* 1 out of 5 emails is spam
* 4 out of 5 emails are ham

$$[
P(\text{Spam}) = 0.20
]$$

$$[
P(\text{Ham}) = 0.80
]$$

These probabilities are our **priors**, since we have not yet looked at the content of the email.

---

## 3ï¸âƒ£ Posterior Probability

The **posterior probability** represents our **updated belief** about an event **after** observing new information (evidence).

### Definition

> **Posterior Probability:** The probability of an event **given that some evidence has been observed**.

Posterior probabilities are always **more informed** than priors.

---

### Example 1: Playing Given It Rains

Now suppose we know that **it is raining today**.

* When it rains, you rarely play.
* Assume you play only 10% of the time when it rains.

$$[
P(\text{Play | Rain}) = 0.10
]$$

Here:

* Prior probability = 67%
* Posterior probability = 10%

The belief changes because we now have **additional information**.

---

### Example 2: Spam Email Given the Word â€œViagraâ€

Suppose an email contains the word **Viagra**.

* Before reading the email:
  $$[P(\text{Spam}) = 0.20]$$

* After seeing the word Viagra, we suspect spam much more strongly.

Assume:

$$[
P(\text{Spam | Viagra}) = 0.70
]$$

Here:

* 20% â†’ **Prior probability**
* 70% â†’ **Posterior probability**

---

## 4ï¸âƒ£ Connecting Priors and Posteriors

The key difference between **prior** and **posterior** probabilities is **information**.

| Type      | Meaning                | Example        |                |
| --------- | ---------------------- | -------------- | -------------- |
| Prior     | Belief before evidence | P(Spam) = 0.20 |                |
| Posterior | Belief after evidence  | P(Spam         | Viagra) = 0.70 |

Bayesian reasoning is all about **updating priors into posteriors** using observed evidence.

---

## 5ï¸âƒ£ Conditional Probability: Formal Definition

Bayesian probability is built on **conditional probability**.

### Formula

$$[
P(A|B) = \frac{P(A \cap B)}{P(B)}, \quad P(B) > 0
]$$

Where:

* A = event of interest (e.g., email is spam)
* B = observed evidence (e.g., word Viagra appears)

---

## 6ï¸âƒ£ Conditional Probability in the Spam Example

Let:

* A = Email is spam
* B = Email contains the word Viagra

We are interested in:

[
P(A|B) = P(\text{Spam | Viagra})
]

* **Prior:** P(A) = 0.20
* **Evidence:** Word Viagra appears
* **Posterior:** P(A|B) = 0.70

This illustrates how Bayesian thinking incorporates **new evidence** to revise beliefs.

---

## 7ï¸âƒ£ Intuition Behind Bayesian Thinking

1. Start with an initial belief (prior)
2. Observe new data or evidence
3. Update the belief to get a posterior
4. Use the posterior as a new prior when more evidence arrives

This cycle repeats continuously.

---

## 8ï¸âƒ£ Why Bayesian Probability Matters

Bayesian probability is widely used in:

* Spam filtering
* Medical diagnosis
* Recommendation systems
* Machine learning
* Risk analysis

Because real-world decisions are almost always made **with partial information**.

---

### âœ… Key Takeaways

* **Prior probability:** Belief before observing evidence
* **Posterior probability:** Updated belief after observing evidence
* Conditional probability provides the mathematical foundation
* Bayesian reasoning = belief updating using evidence

---

In the next step, this naturally leads to **Bayesâ€™ Theorem**, which gives a precise formula for computing posterior probabilities from priors and evidence.

## Introduction to Conditional Probability and Bayesâ€™ Theorem

In this section, we study **conditional probability** and derive **Bayesâ€™ Theorem** using a real-life example from cricket. Conditional probability helps us compute the probability of an event occurring **given that another event has already occurred**.

A commonly quoted cricket statistic is:

> *â€œIndia wins 70% of matches when Sachin Tendulkar scores a century.â€*

This is a classic example of **conditional probability**.

---

## Problem Setup: Cricket Match Scenario

Suppose India plays **100 matches** under the following conditions:

* India **wins 60** matches and **loses 40** matches.
* Sachin Tendulkar plays all 100 matches.
* Sachin scores a **century in 12 matches** and does **not score a century in 88 matches**.

Additional information:

* Out of the **60 matches India wins**, Sachin scores a century in **10** matches.
* Out of the **40 matches India loses**, Sachin scores a century in **2** matches.

---

## Two-Way Contingency Table

|                         | India Wins | India Loses | Total |
| ----------------------- | ---------- | ----------- | ----- |
| Sachin Scores a Century | 10         | 2           | 12    |
| Sachin Doesnâ€™t Score    | 50         | 38          | 88    |
| **Total**               | 60         | 40          | 100   |

This table summarizes all possible combinations of the two events.

---

## Defining the Events

Let:

* **A** = Event that *India wins the match*
* **B** = Event that *Sachin scores a century*

---

## Basic Probabilities

### Probability that India wins

[
P(A) = \frac{60}{100} = 0.6
]

### Probability that Sachin scores a century

$$[
P(B) = \frac{12}{100} = 0.12
]$$

---

## Conditional Probability

### Definition

The conditional probability of event **A given B** is defined as:
$$[
P(A \mid B) = \frac{P(A \cap B)}{P(B)}
]$$

This means: *Given that B has occurred, what is the probability that A occurs?*

---

## Computing Conditional Probability Using the Example

### Probability that India wins given Sachin has scored a century

Out of the **12 matches** where Sachin scored a century:

* India won **10 matches**

$$[
P(A \mid B) = \frac{10}{12}
]$$

This matches the intuitive interpretation: *among matches where Sachin scores a century, India wins 10 out of 12 times.*

---

## Joint Probability

### Definition

The **joint probability** of two events A and B is the probability that **both events occur simultaneously**:
$$[
P(A \cap B)
]$$

### From the table

* Matches where India wins **and** Sachin scores a century = 10

$$[
P(A \cap B) = \frac{10}{100}
]$$

---

## Expressing Joint Probability in Two Ways

### Conditioning on B

$$[
P(A \cap B) = P(A \mid B) \times P(B)
]$$

Interpretation:

* Probability that Sachin scores a century
* Multiplied by probability that India wins **given** Sachin scored a century

---

### Conditioning on A

$$[
P(A \cap B) = P(B \mid A) \times P(A)
]$$

Interpretation:

* Probability that India wins
* Multiplied by probability that Sachin scores a century **given** India wins

From the table:
$$[
P(B \mid A) = \frac{10}{60}
]$$

---

## Derivation of Bayesâ€™ Theorem

Since both expressions describe the same joint probability:

$$[
P(A \mid B)P(B) = P(B \mid A)P(A)
]$$

Rearranging:

$$[
\boxed{P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}}
]$$

This is **Bayesâ€™ Theorem**.

---

## Bayesâ€™ Theorem (General Form)

For any two events A and B such that (P(B) > 0):

$$[
\boxed{P(A \mid B) = \frac{P(B \mid A)P(A)}{P(B)}}
]$$

---

## Interpretation in the Cricket Context

* **Prior Probability**: (P(A)) â€” how often India wins overall
* **Likelihood**: (P(B \mid A)) â€” how often Sachin scores a century when India wins
* **Evidence**: (P(B)) â€” how often Sachin scores a century overall
* **Posterior Probability**: (P(A \mid B)) â€” probability that India wins given Sachin scored a century

---

## Key Takeaways

* Conditional probability focuses on a *restricted sample space*.
* Joint probability can be expressed using conditional probability in two ways.
* Bayesâ€™ Theorem arises by equating these two expressions.
* Bayesâ€™ Theorem allows us to **reverse conditioning** â€” from (P(B \mid A)) to (P(A \mid B)).

---

## Final Answer to the Core Question

**What is the probability that India wins, given that Sachin has scored a century?**

$$[
P(A \mid B) = \frac{10}{12} \approx 0.8333
]$$

So, India wins approximately **83.33%** of the matches when Sachin scores a century.

These notes give a **clear, intuition-first explanation** of **Maximum Likelihood Estimation (MLE)** and **Bayesian Estimation**, using plain language, tiny examples, and then the formal Bayes-rule view.

---

## Big Picture

You are trying to **estimate an unknown parameter** (denoted by (\theta)) using observed data (D).

There are **two philosophies**:

* **MLE**: Let the data decide everything
* **Bayesian**: Combine data with prior belief

---

## Bayesâ€™ Rule (Foundation)

[
p(\theta \mid D) = \frac{p(D \mid \theta), p(\theta)}{p(D)}
]

| Term               | Meaning                         |
| ------------------ | ------------------------------- |
| (p(\theta \mid D)) | Posterior (updated belief)      |
| (p(D \mid \theta)) | Likelihood (fit to data)        |
| (p(\theta))        | Prior (belief before data)      |
| (p(D))             | Evidence (normalizing constant) |

---

## 1. Maximum Likelihood Estimation (MLE)

### Core Idea

> Choose the value of (\theta) that makes the observed data **most likely**.

### Mathematical Definition

[
\hat{\theta}*{\text{MLE}} = \arg\max*{\theta} p(D \mid \theta)
]

### How MLE Thinks

* Treats (\theta) as a **fixed but unknown number**
* Ignores the prior (p(\theta))
* Uses **only likelihood**

### Coin Example (10 tosses)

* Heads = 7, Tails = 3

[
\hat{p}_{\text{MLE}} = \frac{7}{10} = 0.7
]

**Interpretation:**

> â€œThe coin bias is exactly 0.7.â€

### Weakness of MLE

With very little data, MLE becomes **overconfident**.

* 1 toss â†’ Head
* MLE: bias = 1.0 âŒ

---

## 2. Bayesian Estimation

### Core Idea

> Treat the unknown parameter itself as **uncertain**.

### Mathematical View

Bayesian inference computes the **entire posterior distribution**:

[
p(\theta \mid D)
]

not just a single number.

### How Bayesian Thinks

* (\theta) is a **random variable**
* Start with a prior belief (p(\theta))
* Update belief using data

### Coin Example (same data)

* Prior belief: coins are usually fair
* Data: 7 heads out of 10

Bayesian estimate:

[
\mathbb{E}[\theta \mid D] \approx 0.65
]

**Interpretation:**

> â€œThe bias is probably around 0.65, but not certain.â€

---

## 3. Why Bayesian Is Safer with Small Data

### One Toss Example

* Data: 1 Head

| Method   | Estimate |
| -------- | -------- |
| MLE      | 1.0      |
| Bayesian | â‰ˆ 0.67   |

Bayesian avoids extreme conclusions when data is scarce.

---

## 4. How They Treat the Unknown Parameter

| Aspect             | MLE          | Bayesian     |
| ------------------ | ------------ | ------------ |
| Nature of (\theta) | Fixed        | Random       |
| Output             | Single value | Distribution |
| Uncertainty        | Not explicit | Explicit     |

Bayesian estimation **quantifies confidence** using variance of the posterior.

---

## 5. Evidence Term and Computational Cost

The denominator in Bayesâ€™ rule:

[
p(D) = \int p(D \mid \theta) p(\theta) d\theta
]

* Ensures probabilities sum to 1
* Often difficult to compute

### Conjugate Priors

A **conjugate prior** is chosen so that:

* The posterior has the **same functional form** as the prior
* The integral above becomes tractable

Example:

* Binomial likelihood â†’ Beta prior

---

## 6. Bayesian Point Estimates

From the posterior (p(\theta \mid D)), we may choose:

* **Posterior mean** (Bayesian estimate)
* **MAP estimate** (maximum a posteriori)

MAP connects Bayesian estimation to MLE:

* Flat prior â†’ MAP = MLE

---

## 7. Final Comparison Table

| Question                    | MLE            | Bayesian     |
| --------------------------- | -------------- | ------------ |
| Uses prior knowledge?       | No             | Yes          |
| Works well with small data? | Risky          | Safer        |
| Output                      | Point estimate | Distribution |
| Measures uncertainty?       | No             | Yes          |
| Computational cost          | Low            | Higher       |

---

## One-Line Memory Trick

> **MLE listens only to the data.**
> **Bayesian listens to the data with experience and caution.**

---

## Key Takeaway

> MLE fits the data you saw.
> Bayesian inference fits the data **and** your uncertainty about reality.

These notes explain **MLE vs Bayesian estimation** using intuition, stories, and plain language. Math is kept in the background on purpose.

---

## 1. The Core Philosophy: â€œThe Robotâ€ vs â€œThe Humanâ€

You are trying to decide whether a coin is fair.

### Situation

* Flip a coin **3 times**
* Result: **Heads, Heads, Heads**

### MLE (The Literal Robot)

**How it thinks**:

> â€œI only trust what I see.â€

* Observed: 3 heads, 0 tails
* Best explanation of *this data*:

  * Probability of heads = **100%**

**Conclusion**:

> â€œThis coin will always land on heads.â€

**Problem**:

* No common sense
* Extremely overconfident with tiny data

---

### Bayesian (The Skeptical Human)

**How it thinks**:

> â€œThis looks strange, but coins are usually fair.â€

* Data: 3 heads
* Prior belief: coins are rarely magical

**Conclusion**:

> â€œThe coin may be biased, but probably not 100%. Letâ€™s say 60â€“70%.â€

**Key idea**:

* Uses **data + prior experience**

---

## 2. What the Symbols Really Mean (No Fear Version)

The formal notes use Bayesâ€™ rule:

Posterior âˆ Likelihood Ã— Prior

Here is the translation:

* **Î¸ (theta)**: the unknown truth you want to estimate (coin bias)
* **Data**: what you actually observed (H, H, H)

### Plain-English Meaning

> **New belief** depends on **how strong the evidence is** Ã— **what you believed before**

* **Likelihood**: â€œHow well does this guess explain the data?â€
* **Prior**: â€œWhat did I believe before seeing data?â€
* **Posterior**: â€œWhat do I believe now?â€

MLE stops at **likelihood**.
Bayesian uses **likelihood + prior**.

---

## 3. Point Estimate vs Distribution (The Biggest Confusion)

### MLE â†’ Dart ðŸŽ¯

* Gives **one exact number**
* Example: â€œThe bias is 0.7â€

Meaning:

> â€œThis is the answer.â€

---

### Bayesian â†’ Cloud â˜ï¸

* Gives a **range with uncertainty**
* Example: â€œMost likely around 0.65, but could be 0.6 or 0.7â€

Meaning:

> â€œThis is my belief, with confidence levels.â€

---

## 4. Why â€˜Fixedâ€™ vs â€˜Randomâ€™ Matters

This line causes a lot of confusion:

> MLE treats Î¸ as fixed. Bayesian treats Î¸ as random.

### MLE View

* There is **one true value**
* We just donâ€™t know it yet
* With enough data, weâ€™ll find it exactly

### Bayesian View

* We are **never fully certain**
* So we describe our uncertainty using probability
* â€œRandomâ€ means *uncertain*, not changing

---

## 5. Restaurant Review Analogy

A new restaurant has **1 review**, rated **5 stars**.

### MLE User

> â€œAverage = 5.0. Best restaurant ever.â€

âŒ Overconfident because data is tiny

---

### Bayesian User

> â€œMost new places settle around 3.5â€“4 stars. Iâ€™ll assume 4 until more reviews come.â€

âœ… Cautious and realistic

---

## 6. When to Use Which

| Situation             | Better Choice                     |
| --------------------- | --------------------------------- |
| Very small data       | Bayesian                          |
| Large datasets        | MLE or Bayesian (similar results) |
| Need uncertainty      | Bayesian                          |
| Fast, simple estimate | MLE                               |

---

## Final Takeaway

> **MLE trusts the data blindly.**
> **Bayesian trusts the data with experience and caution.**

With lots of data, both agree.
With little data, Bayesian protects you from embarrassment.

## Random Variable â€“ Intuition and Motivation

### Introduction: Casino Probability Concept

You may have heard the famous saying:

> **â€œThe house always wins.â€**

Casinos are designed so that **in the long run**, they make money. While a few players may win big occasionally, most players lose small amounts frequently, and collectively the casino profits.

How do casinos ensure this?

ðŸ‘‰ **By using probability.**

They carefully design games where the *expected outcome* favors the house.

---

## A Simple Probability Game (Casino-Style)

Consider the following game:

* A bag contains **5 balls**:

  * **3 red balls**
  * **2 blue balls**
* A player draws **one ball at a time**, notes the color, and **puts it back** (sampling with replacement).
* This is repeated **4 times**.

### Rules of the Game

* If the player gets **4 red balls**, they **win â‚¹150**.
* Any other outcome â†’ the player **pays â‚¹10**.

Question:

> Is this game profitable for the house in the long run?

To answer this, we follow a **three-step process**.

---

## Three-Step Framework for Analyzing the Game

1. **List all possible outcomes**
2. **Find the probability of each outcome**
3. **Use probabilities to estimate expected profit or loss**

In this lecture, we focus on **Steps 1 and 2**, and introduce an important concept called a **random variable**.

---

## Step 1: Listing All Possible Outcomes

Each draw can be:

* Red (R)
* Blue (B)

Since there are **4 draws**, the total number of outcomes is:

$$[
2^4 = 16
]$$

Examples of outcomes:

* BBBB
* RBBB, BRBB, BBRB, BBBR
* RRBB, RBRB, RBBR, BRRB, BRBR, BBRR
* RRRB, RBRR, BRRR, RRRB
* RRRR

So, there are **16 distinct outcomes**.

---

## Why Outcomes Alone Are Not Enough

Each outcome is a *sequence of colors*, such as:

* RBRB
* BBBR
* RRRR

But working directly with such outcomes is **not convenient** for probability calculations and business decisions.

We need a way to **convert outcomes into numbers**.

ðŸ‘‰ This is where **random variables** come in.

---

## Quantifying Outcomes Using a Random Variable

Let us define a variable:

> **X = number of red balls drawn in 4 draws**

Now each outcome maps to a number:

| Outcome | Number of Red Balls | X |
| ------- | ------------------- | - |
| BBBB    | 0                   | 0 |
| RBBB    | 1                   | 1 |
| RRBB    | 2                   | 2 |
| RRRB    | 3                   | 3 |
| RRRR    | 4                   | 4 |

Instead of tracking 16 outcomes, we now only track **5 possible values of X**:

$$[
X âˆˆ {0,1,2,3,4}
]$$

---

## Definition: Random Variable

A **random variable** is:

> **A function that maps outcomes of a random experiment to numbers.**

Symbolically:

$$[
X : \text{Sample Space} â†’ \mathbb{R}
]$$

In simple words:

> A random variable converts outcomes into something measurable.

---

## Are There Other Ways to Define X?

Yes. We could define:

* X = number of **blue balls** drawn
* X = (number of red balls âˆ’ number of blue balls)

So which definition is correct?

ðŸ‘‰ **It depends on what information you care about.**

---

## Choosing the Right Random Variable

In this game:

* Player wins only when **X = 4**
* Player loses when **X = 0,1,2,3**

So the **number of red balls** is what directly determines:

* Win or loss
* Amount of money gained or lost

Hence, defining:

> **X = number of red balls drawn**

is the most useful choice.

---

## Why Random Variables Are Important

Once outcomes are converted into numbers:

* We can compute **probabilities** of each value of X
* We can compute **expected profit or loss**
* We can design games, policies, and strategies mathematically

This leads us to **probability distributions**, which will be studied next.

---

## Random Variables in Real Life: Banking Example

Suppose you are a data analyst at a bank trying to predict **loan default**.

Original data:

| Customer | Income   | Loan Due | Dependents | Default (Yes/No) |
| -------- | -------- | -------- | ---------- | ---------------- |
| 1        | â‚¹10 lakh | â‚¹75 lakh | 3          | Yes              |
| 2        | â‚¹15 lakh | â‚¹50 lakh | 2          | No               |
| 3        | â‚¹20 lakh | â‚¹40 lakh | 1          | No               |

The target variable is **Yes/No**, which is not numerical.

---

## Defining a Random Variable for Default

Let:

* X = 1 â†’ customer defaults
* X = 0 â†’ customer does not default

Now the data becomes:

| Customer | Income   | Loan Due | Dependents | X |
| -------- | -------- | -------- | ---------- | - |
| 1        | â‚¹10 lakh | â‚¹75 lakh | 3          | 1 |
| 2        | â‚¹15 lakh | â‚¹50 lakh | 2          | 0 |
| 3        | â‚¹20 lakh | â‚¹40 lakh | 1          | 0 |

---

## Why This Conversion Matters

After defining X:

* All data becomes **numerical**
* Statistical models can be applied
* Probabilities and expectations can be computed

This is the foundation of:

* Probability theory
* Statistics
* Machine learning

---

## Final Takeaways

* Random variables **convert outcomes into numbers**
* They make probability calculations practical
* The choice of random variable depends on **what you want to analyze**
* Random variables are the bridge between **real-world problems** and **mathematical analysis**

---

### One-Line Definition (Exam-Ready)

> **A random variable is a function that assigns a real number to each outcome of a random experiment.**


# Probability Distributions (Discrete Random Variables)

## Why we need Probability Distributions

Recall the **three-step process** used to analyze long-run outcomes (casino / business decisions):

1. **List all possible outcomes** of the experiment
2. **Find the probability** of each outcome
3. **Use these probabilities** to compute long-run profit or loss

So far:

* Step 1 is done (we listed all outcomes)
* Step 2 is what we focus on here: **finding probabilities using a random variable**

---

## Step 2: Introducing the Random Variable

In the casino game:

* A bag has **3 red balls** and **2 blue balls**
* A player draws **4 times with replacement**

Instead of working with outcomes like:

```
R B R B
B B R R
R R R R
```

we define a **random variable**:

> **X = Number of red balls drawn in 4 draws**

This converts messy outcomes into numbers we can analyze.

---

## Possible Values of X

Since 4 draws are made:

| Value of X | Meaning             |
| ---------: | ------------------- |
|          0 | No red balls        |
|          1 | Exactly 1 red ball  |
|          2 | Exactly 2 red balls |
|          3 | Exactly 3 red balls |
|          4 | All 4 balls are red |

So, **16 outcomes collapse into just 5 values of X**.

---

## Estimating Probabilities Using an Experiment

Suppose **75 people** played this game.

Each person:

* Draws 4 balls
* We record how many red balls they get

We plot the results:

* **X-axis** â†’ values of X (0 to 4)
* **Y-axis** â†’ frequency (how many people got that value)

This plot is called a **Histogram**.

---

## From Frequency to Probability

Probability is defined as:

> **Probability = (Favourable outcomes) / (Total outcomes)**

Example:

* 26 out of 75 people got **X = 2**

$$[
P(X = 2) = \frac{26}{75} \approx 0.35
]$$

Meaning:

> About **35% of players** will draw exactly 2 red balls.

---

## Probability Distribution Table

Using the same method for all values of X:

| X (No. of red balls) |  P(X) |
| -------------------: | ----: |
|                    0 | 0.027 |
|                    1 | 0.160 |
|                    2 | 0.347 |
|                    3 | 0.333 |
|                    4 | 0.123 |

This table is called the **Probability Distribution of X**.

---

## Probability Distribution (Definition)

A **probability distribution** is **any representation** that shows:

> The probability of **every possible value** of a random variable.

It can be represented as:

### 1. Table

| X | P(X) |
| - | ---- |
| 1 | 1/21 |
| 2 | 2/21 |
| 3 | 3/21 |
| 4 | 4/21 |
| 5 | 5/21 |
| 6 | 6/21 |

### 2. Formula

$$[
P(X = x) = \frac{x}{21}, \quad x = 1,2,3,4,5,6
]$$

### 3. Graph (Bar Chart)

* X-axis â†’ values of X
* Y-axis â†’ probabilities

---

## Important Properties of a Valid Probability Distribution

1. **No probability can be negative**
   $$[
   P(X) \ge 0
   ]$$

2. **Sum of all probabilities must be 1**
   $$[
   \sum P(X) = 1
   ]$$

3. **Shape similarity**:

   * Histogram and probability distribution look the same
   * Only the Y-axis scale changes (frequency â†’ probability)

---

## Business Example: Investment Decision

A company considers investing in a project.

Possible **economic conditions**:

| Economic Cycle | Probability |
| -------------- | ----------: |
| Recession      |         0.1 |
| Normal         |         0.7 |
| Boom           |         0.2 |

This table alone is **not useful** for profit analysis.

---

## Defining a Random Variable

We define:

> **X = Net revenue of the project (â‚¹ crores)**

After analysis, estimated revenues are:

| X (â‚¹ crores) | P(X) |
| -----------: | ---: |
|         -305 |  0.1 |
|          +15 |  0.7 |
|          +95 |  0.2 |

Now we have a **probability distribution of profit/loss**.

---

## Why This Matters

Once we have a probability distribution:

* We can compute **expected profit**
* We can decide whether the project is worth investing in

ðŸ‘‰ This leads directly to the concept of **Expected Value**, which answers:

> *On average, will we make money or lose money in the long run?*

---

## One-Line Memory Rule

> **Random Variable â†’ Probability Distribution â†’ Expected Value â†’ Decision**

This is the backbone of probability in:

* Casinos
* Finance
* Machine Learning
* Risk analysis

# Expected Value (EV)

## Why Expected Value is Needed

Recall the **three-step framework** used to analyze longâ€‘run outcomes:

1. Find all possible outcomes
2. Find the probability of each outcome
3. **Use probabilities to estimate longâ€‘run profit or loss**

Steps 1 and 2 give us a **probability distribution**.

ðŸ‘‰ **Expected Value** is the tool that completes Step 3.

---

## Intuition: What Expected Value Really Means

Expected Value answers this question:

> **If this experiment is repeated a very large number of times, what is the average outcome per experiment?**

Important:

* Expected value **need not be a value that actually occurs**
* It represents a **longâ€‘run average**, not a single outcome

---

## Expected Value Using the Red Ball Game

### Random Variable Definition

Let:

> **X = Number of red balls drawn in one game**

Possible values of X:

| X | Meaning         |
| - | --------------- |
| 0 | No red balls    |
| 1 | One red ball    |
| 2 | Two red balls   |
| 3 | Three red balls |
| 4 | Four red balls  |

Corresponding probability distribution:

| X | P(X)  |
| - | ----- |
| 0 | 0.027 |
| 1 | 0.160 |
| 2 | 0.347 |
| 3 | 0.333 |
| 4 | 0.133 |

---

## Expected Value via Large Number of Players

Assume **1000 players** play the game.

Expected number of players for each outcome:

| X | Expected Players |
| - | ---------------- |
| 0 | 27               |
| 1 | 160              |
| 2 | 347              |
| 3 | 333              |
| 4 | 133              |

Total number of red balls observed:

$$[
0Â·27 + 1Â·160 + 2Â·347 + 3Â·333 + 4Â·133 = 2385
]$$

Average red balls per game:

$$[
\frac{2385}{1000} = 2.385
]$$

---

## Formal Definition of Expected Value

Let a random variable X take values:

$$[x_1, x_2, ..., x_n]$$

with probabilities:

$$[P(X = x_1), P(X = x_2), ..., P(X = x_n)]$$

Then the **expected value of X** is:

$$[
\boxed{E[X] = x_1P(X=x_1) + x_2P(X=x_2) + ... + x_nP(X=x_n)}
]$$

---

## Expected Value for Red Balls

$$[
E[X] = 0(0.027) + 1(0.160) + 2(0.347) + 3(0.333) + 4(0.133) = 2.385
]$$

Interpretation:

> Over a very large number of games, the **average number of red balls per game is 2.385**.

---

## Important Clarification

You will **never** see 2.385 red balls in one game.

Expected value:

* Is **not a prediction of a single outcome**
* Is a **longâ€‘run average** over infinitely many repetitions

---

## Choosing the Right Random Variable

So far:

* X = number of red balls

But our **real question** is:

> Will the **house make money or lose money**?

So we redefine X.

---

## Random Variable for Profit Analysis

Let:

> **X = Money won by the player in one game (â‚¹)**

Possible values:

| Outcome      | X (â‚¹) | Probability |
| ------------ | ----- | ----------- |
| Player wins  | +150  | 0.133       |
| Player loses | âˆ’10   | 0.867       |

---

## Expected Value of Playerâ€™s Winnings

$$[
E[X] = 150(0.133) + (-10)(0.867)
]$$

$$[
E[X] = 19.95 - 8.67 = +11.28
]$$

Interpretation:

> On average, a player **wins â‚¹11.28 per game**.

---

## Why the House Loses Money Here

Since:

$$[
E[X] > 0
]$$

* Players win on average
* House loses money in the long run

---

## How Casinos Ensure Profit

Casinos design games so that:

> **Expected value of playerâ€™s winnings is negative**

They achieve this by:

* Reducing prize money
* Increasing penalty
* Lowering probability of winning

---

## Classic Example: Dice Roll

Define:

> **X = number on a fair die**

Expected value:

$$[
E[X] = \frac{1+2+3+4+5+6}{6} = 3.5
]$$

Even though 3.5 never appears, it is the **longâ€‘run average**.

---

## Business Example: Investment Decision

Probability distribution of project return:

| X (â‚¹ crores) | P(X) |
| ------------ | ---- |
| âˆ’305         | 0.1  |
| +15          | 0.7  |
| +95          | 0.2  |

Expected return:

$$[
E[X] = (-305)(0.1) + (15)(0.7) + (95)(0.2)
]$$

$$[
E[X] = -30.5 + 10.5 + 19 = -1
]$$

Interpretation:

> Expected loss = â‚¹1 crore â†’ **Do not invest**

---

## Final Takeaways

* Expected value measures **longâ€‘run average outcome**
* It is the key tool for **decisionâ€‘making under uncertainty**
* Positive EV â†’ favorable
* Negative EV â†’ unfavorable

---

## Oneâ€‘Line Memory Rule

> **Expected Value tells you who wins in the long run, not in one round.**
